{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs9E_R5yD48u"
      },
      "source": [
        "# **Assignment \\#2**: Machine Learning MC886/MO444\n",
        "University of Campinas (UNICAMP), Institute of Computing (IC)\n",
        "\n",
        "Prof. Sandra Avila, 2022s2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFS9Oum_RJX9",
        "outputId": "45094cce-bb6b-4ad6-966e-434b6c7b9db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RA1: 213259 Arthur Baia\n",
            "RA2: 200025 José Afonso\n"
          ]
        }
      ],
      "source": [
        "# TODO: RA & Name\n",
        "print(f'RA1: 213259 ' + 'Arthur Baia')\n",
        "print(f'RA2: 200025 ' + 'José Afonso')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVGH2s7fD_03"
      },
      "source": [
        "## Objective\n",
        "\n",
        "Explore **linear regression** and **logistic regression** alternatives and come up with the best possible model for the problems, avoiding overfitting. In particular, predict the performance of students from public schools in the state of São Paulo based on socioeconomic data from SARESP (School Performance Assessment System of the State of São Paulo, or Sistema de Avaliação de Rendimento Escolar do Estado de São Paulo) 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3XDZRGqEwsk"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "These data were aggregated from [Open Data Platform of the Secretary of Education of the State of São Paulo](https://dados.educacao.sp.gov.br/) (*Portal de Dados Abertos da Secretaria da Educação do Estado de São Paulo*). The dataset is based on two data sources: [SARESP questionnaire](https://dados.educacao.sp.gov.br/dataset/question%C3%A1rios-saresp) and [SARESP test](https://dados.educacao.sp.gov.br/dataset/profici%C3%AAncia-do-sistema-de-avalia%C3%A7%C3%A3o-de-rendimento-escolar-do-estado-de-s%C3%A3o-paulo-saresp-por), conducted in 2021 with students from the 5th and 9th year of Primary School and 3rd year of Highschool. The questionnaire comprehends 63 socio-economical questions, and it is available at the [link](https://dados.educacao.sp.gov.br/sites/default/files/Saresp_Quest_2021_Perguntas_Alunos.pdf ) ([English version](https://docs.google.com/document/d/1GUax3wwYxA43d3iNOiyCRImeCHgx8vUJrHlSzzYIXA4/edit?usp=sharing)), and the test is composed of questions of Portuguese, Mathematics, and Natural Sciences.\n",
        "\n",
        "\n",
        "**Data Dictionary**:\n",
        "\n",
        "- **CD_ALUNO**: Student ID;\n",
        "\n",
        "- **CODESC**: School ID;\n",
        "\n",
        "- **NOMESC**: School Name;\n",
        "\n",
        "- **RegiaoMetropolitana**: Metropolitan region;\n",
        "\n",
        "- **DE**: Name of the Education Board;\n",
        "\n",
        "- **CODMUN**: City ID;\n",
        "\n",
        "- **MUN**: City name;\n",
        "\n",
        "- **SERIE_ANO**: Scholar year;\n",
        "\n",
        "- **TURMA**: Class;\n",
        "\n",
        "- **TP_SEXO**: Sex (Female/Male);\n",
        "\n",
        "- **DT_NASCIMENTO**: Birth date;\n",
        "\n",
        "- **PERIODO**: Period of study (morning, afternoon, evening);\n",
        "\n",
        "- **Tem_Nec**: Whether student has any special needs (1 = yes, 0 = no);\n",
        "\n",
        "- **NEC_ESP_1** - **NEC_ESP_5**: Student disabilities;\n",
        "\n",
        "- **Tipo_PROVA**: Exam type (A = Enlarged, B = Braile, C = Common);\n",
        "\n",
        "- **QN**: Student answer to the question N (N= 1, ... , 63), see  questions in [questionnaire](https://dados.educacao.sp.gov.br/sites/default/files/Saresp_Quest_2021_Perguntas_Alunos.pdf ) ([English version](https://docs.google.com/document/d/1GUax3wwYxA43d3iNOiyCRImeCHgx8vUJrHlSzzYIXA4/edit?usp=sharing));\n",
        "\n",
        "- **porc_ACERT_lp**: Percentage of correct answers in the Portuguese test;\n",
        "\n",
        "- **porc_ACERT_MAT**: Percentage of correct answers in the Mathematics test;\n",
        "\n",
        "- **porc_ACERT_CIE**: Percentage of correct answers in the Natural Sciences test;\n",
        "\n",
        "- **nivel_profic_lp**: Proficiency level in the Portuguese test;\n",
        "\n",
        "- **nivel_profic_mat**: Proficiency level in the Mathematics test;\n",
        "\n",
        "- **nivel_profic_cie**:  Proficiency level in the Natural Sciences test.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "You must respect the following training/test split:\n",
        "- SARESP_train.csv\n",
        "- SARESP_test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAA8hsZUseO"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "This part of the assignment aims to predict students' performance on Portuguese, Mathematics, and Natural Sciences tests (target values: `porc_ACERT_lp`, `porc_ACERT_MAT`, and  `porc_ACERT_CIE`) based on their socioeconomic data. Then, at this point, you have to **drop the columns `nivel_profic_lp`, `nivel_profic_mat`** and **`nivel_profic_cie`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d495CmpCltx"
      },
      "source": [
        "### Activities\n",
        "\n",
        "1. (3.5 points) Perform Linear Regression. You should implement your solution and compare it with ```sklearn.linear_model.SGDRegressor``` (linear model fitted by minimizing a regularized empirical loss with SGD, http://scikit-learn.org). Keep in mind that friends don't let friends use testing data for training :-)\n",
        "\n",
        "Note: Before we start an ML project, we always conduct a brief exploratory analysis :D \n",
        "\n",
        "Some factors to consider: Are there any outliers? Are there missing values? How will you handle categorical variables? Are there any features with low correlation with the target variables? What happens if you drop them?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Step 1: Load data and check it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0QxxH1KgE1",
        "outputId": "c80087fa-d4a6-4cfb-dbb8-e0ff167ae8f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load and preprocess your dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv(\"SARESP_train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "DAADl99uoJp1",
        "outputId": "aecc2d5c-d89c-4b7f-c957-cfe195d94a0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CD_ALUNO</th>\n",
              "      <th>NOMESC</th>\n",
              "      <th>Q1</th>\n",
              "      <th>Q2</th>\n",
              "      <th>Q3</th>\n",
              "      <th>Q4</th>\n",
              "      <th>Q5</th>\n",
              "      <th>Q6</th>\n",
              "      <th>Q7</th>\n",
              "      <th>Q8</th>\n",
              "      <th>...</th>\n",
              "      <th>NEC_ESP_4</th>\n",
              "      <th>NEC_ESP_5</th>\n",
              "      <th>Tipo_PROVA</th>\n",
              "      <th>Tem_Nec</th>\n",
              "      <th>porc_ACERT_lp</th>\n",
              "      <th>porc_ACERT_MAT</th>\n",
              "      <th>porc_ACERT_CIE</th>\n",
              "      <th>nivel_profic_lp</th>\n",
              "      <th>nivel_profic_mat</th>\n",
              "      <th>nivel_profic_cie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26270013</td>\n",
              "      <td>JULIO FORTES</td>\n",
              "      <td>B</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>41.7</td>\n",
              "      <td>20.8</td>\n",
              "      <td>20.8</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30756614</td>\n",
              "      <td>MESSIAS FREIRE PROFESSOR</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>83.3</td>\n",
              "      <td>100.0</td>\n",
              "      <td>66.7</td>\n",
              "      <td>Adequado</td>\n",
              "      <td>Avançado</td>\n",
              "      <td>Adequado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26014872</td>\n",
              "      <td>JOSE CONTI</td>\n",
              "      <td>B</td>\n",
              "      <td>E</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>58.3</td>\n",
              "      <td>37.5</td>\n",
              "      <td>54.2</td>\n",
              "      <td>Básico</td>\n",
              "      <td>Básico</td>\n",
              "      <td>Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25739025</td>\n",
              "      <td>NAPOLEAO DE CARVALHO FREIRE PROFESSOR</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>29.2</td>\n",
              "      <td>29.2</td>\n",
              "      <td>16.7</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27363009</td>\n",
              "      <td>RESIDENCIAL BORDON</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>79.2</td>\n",
              "      <td>41.7</td>\n",
              "      <td>50.0</td>\n",
              "      <td>Adequado</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120591</th>\n",
              "      <td>28799794</td>\n",
              "      <td>ENNIO CHIESA PROFESSOR</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>79.2</td>\n",
              "      <td>66.7</td>\n",
              "      <td>83.3</td>\n",
              "      <td>Adequado</td>\n",
              "      <td>Básico</td>\n",
              "      <td>Adequado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120592</th>\n",
              "      <td>27825068</td>\n",
              "      <td>HELIO HELENE</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>37.5</td>\n",
              "      <td>25.0</td>\n",
              "      <td>16.7</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120593</th>\n",
              "      <td>23873470</td>\n",
              "      <td>ALBERTO SANTOS DUMONT</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>37.5</td>\n",
              "      <td>41.7</td>\n",
              "      <td>Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120594</th>\n",
              "      <td>31376275</td>\n",
              "      <td>FRANCISCO BONFIM</td>\n",
              "      <td>B</td>\n",
              "      <td>E</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>45.8</td>\n",
              "      <td>70.8</td>\n",
              "      <td>54.2</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120595</th>\n",
              "      <td>28109335</td>\n",
              "      <td>MAGDALENA SANSEVERINO GROSSO PROFESSORA</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>37.5</td>\n",
              "      <td>16.7</td>\n",
              "      <td>16.7</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "      <td>Abaixo do Básico</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120596 rows × 88 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        CD_ALUNO                                   NOMESC Q1 Q2 Q3 Q4 Q5 Q6  \\\n",
              "0       26270013                             JULIO FORTES  B  E  E  E  E  E   \n",
              "1       30756614                 MESSIAS FREIRE PROFESSOR  B  D  E  C  E  E   \n",
              "2       26014872                               JOSE CONTI  B  E  B  D  E  B   \n",
              "3       25739025    NAPOLEAO DE CARVALHO FREIRE PROFESSOR  B  D  E  D  C  E   \n",
              "4       27363009                       RESIDENCIAL BORDON  B  D  E  E  E  E   \n",
              "...          ...                                      ... .. .. .. .. .. ..   \n",
              "120591  28799794                   ENNIO CHIESA PROFESSOR  A  E  E  E  E  E   \n",
              "120592  27825068                             HELIO HELENE  B  D  D  D  D  D   \n",
              "120593  23873470                    ALBERTO SANTOS DUMONT  A  E  E  E  E  E   \n",
              "120594  31376275                         FRANCISCO BONFIM  B  E  C  C  D  B   \n",
              "120595  28109335  MAGDALENA SANSEVERINO GROSSO PROFESSORA  A  A  A  A  A  A   \n",
              "\n",
              "       Q7 Q8  ... NEC_ESP_4 NEC_ESP_5 Tipo_PROVA Tem_Nec porc_ACERT_lp  \\\n",
              "0       E  E  ...       NaN       NaN          C       0          41.7   \n",
              "1       E  E  ...       NaN       NaN          C       0          83.3   \n",
              "2       D  C  ...       NaN       NaN          C       0          58.3   \n",
              "3       D  D  ...       NaN       NaN          C       0          29.2   \n",
              "4       E  C  ...       NaN       NaN          C       0          79.2   \n",
              "...    .. ..  ...       ...       ...        ...     ...           ...   \n",
              "120591  E  E  ...       NaN       NaN          C       0          79.2   \n",
              "120592  D  D  ...       NaN       NaN          C       0          37.5   \n",
              "120593  D  D  ...       NaN       NaN          C       0          50.0   \n",
              "120594  B  A  ...       NaN       NaN          C       1          45.8   \n",
              "120595  A  A  ...       NaN       NaN          C       1          37.5   \n",
              "\n",
              "       porc_ACERT_MAT porc_ACERT_CIE   nivel_profic_lp  nivel_profic_mat  \\\n",
              "0                20.8           20.8  Abaixo do Básico  Abaixo do Básico   \n",
              "1               100.0           66.7          Adequado          Avançado   \n",
              "2                37.5           54.2            Básico            Básico   \n",
              "3                29.2           16.7  Abaixo do Básico  Abaixo do Básico   \n",
              "4                41.7           50.0          Adequado  Abaixo do Básico   \n",
              "...               ...            ...               ...               ...   \n",
              "120591           66.7           83.3          Adequado            Básico   \n",
              "120592           25.0           16.7  Abaixo do Básico  Abaixo do Básico   \n",
              "120593           37.5           41.7            Básico  Abaixo do Básico   \n",
              "120594           70.8           54.2  Abaixo do Básico            Básico   \n",
              "120595           16.7           16.7  Abaixo do Básico  Abaixo do Básico   \n",
              "\n",
              "        nivel_profic_cie  \n",
              "0       Abaixo do Básico  \n",
              "1               Adequado  \n",
              "2                 Básico  \n",
              "3       Abaixo do Básico  \n",
              "4                 Básico  \n",
              "...                  ...  \n",
              "120591          Adequado  \n",
              "120592  Abaixo do Básico  \n",
              "120593  Abaixo do Básico  \n",
              "120594  Abaixo do Básico  \n",
              "120595  Abaixo do Básico  \n",
              "\n",
              "[120596 rows x 88 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePztAuVOoMe1",
        "outputId": "bbde7cd6-825a-4234-ea16-40b77629606e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CD_ALUNO</th>\n",
              "      <th>CODMUN</th>\n",
              "      <th>CODESC</th>\n",
              "      <th>NEC_ESP_5</th>\n",
              "      <th>Tem_Nec</th>\n",
              "      <th>porc_ACERT_lp</th>\n",
              "      <th>porc_ACERT_MAT</th>\n",
              "      <th>porc_ACERT_CIE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.205960e+05</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.734087e+07</td>\n",
              "      <td>364.349075</td>\n",
              "      <td>279415.870510</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>60.151213</td>\n",
              "      <td>52.225829</td>\n",
              "      <td>56.928877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.946464e+06</td>\n",
              "      <td>220.098318</td>\n",
              "      <td>394245.824543</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.139376</td>\n",
              "      <td>21.730825</td>\n",
              "      <td>21.262466</td>\n",
              "      <td>18.441383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.739548e+07</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.529711e+07</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>15568.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>41.700000</td>\n",
              "      <td>37.500000</td>\n",
              "      <td>45.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.712102e+07</td>\n",
              "      <td>336.000000</td>\n",
              "      <td>35178.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>62.500000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>58.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.910558e+07</td>\n",
              "      <td>582.000000</td>\n",
              "      <td>901573.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>79.200000</td>\n",
              "      <td>66.700000</td>\n",
              "      <td>70.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.796186e+07</td>\n",
              "      <td>793.000000</td>\n",
              "      <td>926103.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           CD_ALUNO         CODMUN         CODESC  NEC_ESP_5        Tem_Nec  \\\n",
              "count  1.205960e+05  120596.000000  120596.000000        0.0  120596.000000   \n",
              "mean   2.734087e+07     364.349075  279415.870510        NaN       0.019818   \n",
              "std    2.946464e+06     220.098318  394245.824543        NaN       0.139376   \n",
              "min    1.739548e+07     100.000000      24.000000        NaN       0.000000   \n",
              "25%    2.529711e+07     100.000000   15568.000000        NaN       0.000000   \n",
              "50%    2.712102e+07     336.000000   35178.000000        NaN       0.000000   \n",
              "75%    2.910558e+07     582.000000  901573.000000        NaN       0.000000   \n",
              "max    3.796186e+07     793.000000  926103.000000        NaN       1.000000   \n",
              "\n",
              "       porc_ACERT_lp  porc_ACERT_MAT  porc_ACERT_CIE  \n",
              "count  120596.000000   120596.000000   120596.000000  \n",
              "mean       60.151213       52.225829       56.928877  \n",
              "std        21.730825       21.262466       18.441383  \n",
              "min         0.000000        0.000000        0.000000  \n",
              "25%        41.700000       37.500000       45.800000  \n",
              "50%        62.500000       50.000000       58.300000  \n",
              "75%        79.200000       66.700000       70.800000  \n",
              "max       100.000000      100.000000      100.000000  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvxhHEeSpaW0",
        "outputId": "7820214f-75cb-4153-bd20-19b11ec475d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'NOMESC', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8',\n",
              "       'Q9', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18',\n",
              "       'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28',\n",
              "       'Q29', 'Q30', 'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38',\n",
              "       'Q39', 'Q40', 'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48',\n",
              "       'Q49', 'Q50', 'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58',\n",
              "       'Q59', 'Q60', 'Q61', 'Q62', 'Q63', 'RegiaoMetropolitana', 'DE',\n",
              "       'CODMUN', 'MUN', 'CODESC', 'SERIE_ANO', 'TURMA', 'TP_SEXO',\n",
              "       'DT_NASCIMENTO', 'PERIODO', 'NEC_ESP_1', 'NEC_ESP_2', 'NEC_ESP_3',\n",
              "       'NEC_ESP_4', 'NEC_ESP_5', 'Tipo_PROVA', 'Tem_Nec', 'porc_ACERT_lp',\n",
              "       'porc_ACERT_MAT', 'porc_ACERT_CIE', 'nivel_profic_lp',\n",
              "       'nivel_profic_mat', 'nivel_profic_cie'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Step 2: Check NaNs in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUu_-HXQs_tb",
        "outputId": "9578fccd-4ac9-4664-a6b4-021b02a17856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NEC_ESP_5           120596\n",
              "NEC_ESP_4           120595\n",
              "NEC_ESP_3           120520\n",
              "NEC_ESP_2           120489\n",
              "NEC_ESP_1           118206\n",
              "                     ...  \n",
              "Q26                      0\n",
              "Q25                      0\n",
              "Q24                      0\n",
              "Q23                      0\n",
              "nivel_profic_cie         0\n",
              "Length: 88, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum(axis=0).sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there are a lot of NaN's in the NEC_ESP columns, they will be dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'NOMESC', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8',\n",
              "       'Q9', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18',\n",
              "       'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28',\n",
              "       'Q29', 'Q30', 'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38',\n",
              "       'Q39', 'Q40', 'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48',\n",
              "       'Q49', 'Q50', 'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58',\n",
              "       'Q59', 'Q60', 'Q61', 'Q62', 'Q63', 'RegiaoMetropolitana', 'DE',\n",
              "       'CODMUN', 'MUN', 'CODESC', 'SERIE_ANO', 'TURMA', 'TP_SEXO',\n",
              "       'DT_NASCIMENTO', 'PERIODO', 'Tipo_PROVA', 'Tem_Nec', 'porc_ACERT_lp',\n",
              "       'porc_ACERT_MAT', 'porc_ACERT_CIE', 'nivel_profic_lp',\n",
              "       'nivel_profic_mat', 'nivel_profic_cie'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def drop_nec_columns(df):\n",
        "    for i in range(1, 6):\n",
        "        df.drop(columns=[f'NEC_ESP_{i}'], inplace=True)\n",
        "drop_nec_columns(df)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Step 3: Transform data\n",
        "\n",
        "Based on the questions {Q0, .., Q63} and their meaning and possible values, it's needed to transform them into numerical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'NOMESC', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8',\n",
              "       'Q9', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18',\n",
              "       'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28',\n",
              "       'Q29', 'Q30', 'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38',\n",
              "       'Q39', 'Q40', 'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48',\n",
              "       'Q49', 'Q50', 'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58',\n",
              "       'Q59', 'Q60', 'Q61', 'Q62', 'RegiaoMetropolitana', 'DE', 'CODMUN',\n",
              "       'MUN', 'CODESC', 'SERIE_ANO', 'TURMA', 'TP_SEXO', 'DT_NASCIMENTO',\n",
              "       'PERIODO', 'Tipo_PROVA', 'Tem_Nec', 'porc_ACERT_lp', 'porc_ACERT_MAT',\n",
              "       'porc_ACERT_CIE', 'nivel_profic_lp', 'nivel_profic_mat',\n",
              "       'nivel_profic_cie', 'Q63_A', 'Q63_B', 'Q63_C', 'Q63_D'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def question_map(df):\n",
        "\n",
        "    ordinal_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5}\n",
        "    inversed_ordinal_map = {'A': 5, 'B': 4, 'C': 3, 'D': 2, 'E': 1}\n",
        "    dont_know_ordinal_map = {'A': 1, 'B': 2, 'C': 3, 'D': 1, 'E': 2}\n",
        "    inversed_dont_know_ordinal_map = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'E': 2} #applied penalty for don't know anwser\n",
        "    home_ordinal_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
        "    miss_someone_map = {'A': 1, 'B': 1, 'C': 1, 'D': 0}\n",
        "    someone_helped_map = {'A': 2, 'B': 1, 'C': 1, 'D': 0}\n",
        "\n",
        "    for q in range(1, 64):\n",
        "        if (q <= 8 or (q > 26 and q <= 33) or (q > 33 and q <= 41) or (q > 56 and q <= 58) or q == 60): #ordinal map\n",
        "            df.replace({f'Q{q}': ordinal_map}, inplace=True)\n",
        "        elif ((q > 8 and q <= 26) or q == 59):\n",
        "            df.replace({f'Q{q}': inversed_ordinal_map}, inplace=True) \n",
        "        elif (q == 42):\n",
        "            df.replace({f'Q{q}': inversed_dont_know_ordinal_map}, inplace=True)\n",
        "        elif (q > 42 and q <= 49):\n",
        "            df.replace({f'Q{q}': dont_know_ordinal_map}, inplace=True)\n",
        "        elif (q > 49 and q <= 56):\n",
        "            df.replace({f'Q{q}': home_ordinal_map}, inplace=True)\n",
        "        elif (q == 61):\n",
        "            df.replace({f'Q{q}': miss_someone_map}, inplace=True)\n",
        "        elif (q == 62):\n",
        "            df.replace({f'Q{q}': someone_helped_map}, inplace=True)\n",
        "        else: # 63 question\n",
        "            df = pd.concat([df, pd.get_dummies(df['Q63'], prefix='Q63')], axis=1)\n",
        "            df.drop(columns=[ 'Q63'], inplace=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "df = question_map(df)\n",
        "df.columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Considering there are many columns related to geographical columns, { 'RegiaoMetropolitana', 'DE', 'CODMUN', 'MUN', 'CODESC' }, we will select only one column related to this \n",
        "matter, because there are no indicator related to social geographical like HDI to correlate to them. Also, if we one hot encode each of these columns there will be a huge increase in dimensionality. So, we will choose the 'Regiao metropolitana' column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RegiaoMetropolitana\n",
              "Interior                                                   39211\n",
              "Região Metropolitana da Baixada Santista                    7688\n",
              "Região Metropolitana de Campinas                            3465\n",
              "Região Metropolitana de Ribeirão Preto                      6712\n",
              "Região Metropolitana de Sorocaba                            6736\n",
              "Região Metropolitana de São Paulo                          47437\n",
              "Região Metropolitana do Vale do Paraíba e Litoral Norte     9347\n",
              "Name: RegiaoMetropolitana, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['RegiaoMetropolitana'].groupby(df['RegiaoMetropolitana']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_geo(df):\n",
        "    df = pd.concat([df, pd.get_dummies(df['RegiaoMetropolitana'], prefix='RegiaoMetropolitana')], axis=1)\n",
        "    df.drop(columns=['NOMESC', 'MUN', 'CODESC', 'CODMUN', 'RegiaoMetropolitana',  'DE'], inplace=True)\n",
        "    return df\n",
        "df = select_geo(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10',\n",
              "       'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20',\n",
              "       'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30',\n",
              "       'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40',\n",
              "       'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50',\n",
              "       'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60',\n",
              "       'Q61', 'Q62', 'Tem_Nec', 'porc_ACERT_lp', 'porc_ACERT_MAT',\n",
              "       'porc_ACERT_CIE', 'Q63_A', 'Q63_B', 'Q63_C', 'Q63_D',\n",
              "       'RegiaoMetropolitana_Interior',\n",
              "       'RegiaoMetropolitana_Região Metropolitana da Baixada Santista',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Campinas',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Sorocaba',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de São Paulo',\n",
              "       'RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df._get_numeric_data().columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['nivel_profic_mat',\n",
              " 'SERIE_ANO',\n",
              " 'DT_NASCIMENTO',\n",
              " 'nivel_profic_cie',\n",
              " 'nivel_profic_lp',\n",
              " 'PERIODO',\n",
              " 'Tipo_PROVA',\n",
              " 'TURMA',\n",
              " 'TP_SEXO']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_categorical_columns(df):\n",
        "    cols = df.columns\n",
        "\n",
        "    num_cols = df._get_numeric_data().columns\n",
        "\n",
        "    return list(set(cols) - set(num_cols))\n",
        "get_categorical_columns(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since birth date is not a numerical category, we turn it into age (idade column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         18\n",
              "1         12\n",
              "2         15\n",
              "3         18\n",
              "4         15\n",
              "          ..\n",
              "120591    16\n",
              "120592    13\n",
              "120593    19\n",
              "120594    12\n",
              "120595    17\n",
              "Name: idade, Length: 120596, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "def calculate_age(born):\n",
        "    born = datetime.strptime(born, \"%m/%d/%Y\").date()\n",
        "    today = date.today()\n",
        "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
        "\n",
        "\n",
        "df['idade'] = df['DT_NASCIMENTO'].apply(calculate_age)\n",
        "df = df.drop(columns=['DT_NASCIMENTO'])\n",
        "df['idade']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='idade'>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df['idade'].groupby(df['idade']).count().plot(kind='bar')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mapping categorical features into numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sex_mapper(df):\n",
        "    sex_map = {'F': 0, 'M': 1}\n",
        "    df.replace({'TP_SEXO': sex_map}, inplace=True)\n",
        "    return df\n",
        "df = sex_mapper(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since more years of schooling may mean more knowledge, we will choose to make SERIE_ANO as ordinal as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         3\n",
              "1         1\n",
              "2         2\n",
              "3         3\n",
              "4         2\n",
              "         ..\n",
              "120591    2\n",
              "120592    1\n",
              "120593    3\n",
              "120594    1\n",
              "120595    2\n",
              "Name: SERIE_ANO, Length: 120596, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def school_year_mapper(df): \n",
        "    school_year_map = {'EM-3ª série': 3, '9º Ano EF': 2, '5º Ano EF': 1}\n",
        "    df.replace({'SERIE_ANO': school_year_map}, inplace=True)\n",
        "    return df\n",
        "df = school_year_mapper(df)\n",
        "df['SERIE_ANO']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Drop column TURMA, since in large scale it is not relevant\"\"\"\n",
        "def drop_turma(df):\n",
        "    df = df.drop(columns=['TURMA'])\n",
        "    return df\n",
        "df = drop_turma(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"One hot encoding on the columns PERIODO, Tipo_PROVA\"\"\"\n",
        "def one_hot_encoding_(df):\n",
        "    df = pd.concat([df, pd.get_dummies(df['PERIODO'], prefix='PERIODO')], axis=1)\n",
        "    df = df.drop(columns=['PERIODO'])\n",
        "    df = pd.concat([df, pd.get_dummies(df['Tipo_PROVA'], prefix='Tipo_PROVA')], axis=1)\n",
        "    df = df.drop(columns=['Tipo_PROVA'])\n",
        "    return df\n",
        "df = one_hot_encoding_(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['nivel_profic_lp', 'nivel_profic_mat', 'nivel_profic_cie']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_categorical_columns(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Abaixo do Básico' 'Avançado' 'Básico' 'Adequado']\n",
            "['Abaixo do Básico' 'Adequado' 'Básico' 'Avançado']\n",
            "['Abaixo do Básico' 'Adequado' 'Básico' 'Avançado']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Get unique values of nivel_profic_mat\"\"\"\n",
        "print(df['nivel_profic_mat'].unique())\n",
        "print(df['nivel_profic_lp'].unique())\n",
        "print(df['nivel_profic_cie'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         0\n",
              "1         3\n",
              "2         1\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "120591    1\n",
              "120592    0\n",
              "120593    0\n",
              "120594    1\n",
              "120595    0\n",
              "Name: nivel_profic_mat, Length: 120596, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def map_categorical_columns_for_log_reg(df):\n",
        "    \"\"\"Map the categorical columns to numerical values\"\"\"\n",
        "    map_for_log_reg = { 'Abaixo do Básico': 0, 'Adequado': 2, 'Básico': 1, 'Avançado': 3}\n",
        "    df.replace({'nivel_profic_mat': map_for_log_reg}, inplace=True)\n",
        "    df.replace({'nivel_profic_lp': map_for_log_reg}, inplace=True)\n",
        "    df.replace({'nivel_profic_cie': map_for_log_reg}, inplace=True)\n",
        "    return df\n",
        "map_categorical_columns_for_log_reg(df)\n",
        "df['nivel_profic_mat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_categorical_columns(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the columns were rightly transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Step 4: With all the columns being numbers, we can start to perform numerical analysis such as correlation, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CD_ALUNO</th>\n",
              "      <th>Q1</th>\n",
              "      <th>Q2</th>\n",
              "      <th>Q3</th>\n",
              "      <th>Q4</th>\n",
              "      <th>Q5</th>\n",
              "      <th>Q6</th>\n",
              "      <th>Q7</th>\n",
              "      <th>Q8</th>\n",
              "      <th>Q9</th>\n",
              "      <th>...</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Sorocaba</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de São Paulo</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte</th>\n",
              "      <th>idade</th>\n",
              "      <th>PERIODO_MANHÃ</th>\n",
              "      <th>PERIODO_NOITE</th>\n",
              "      <th>PERIODO_TARDE</th>\n",
              "      <th>Tipo_PROVA_A</th>\n",
              "      <th>Tipo_PROVA_C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.205960e+05</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "      <td>120596.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.734087e+07</td>\n",
              "      <td>1.803567</td>\n",
              "      <td>4.183397</td>\n",
              "      <td>4.234850</td>\n",
              "      <td>3.901249</td>\n",
              "      <td>3.994676</td>\n",
              "      <td>3.839248</td>\n",
              "      <td>4.015813</td>\n",
              "      <td>3.542456</td>\n",
              "      <td>4.551519</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.055856</td>\n",
              "      <td>0.393355</td>\n",
              "      <td>0.077507</td>\n",
              "      <td>15.735464</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.110169</td>\n",
              "      <td>0.207478</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.999088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.946464e+06</td>\n",
              "      <td>0.634012</td>\n",
              "      <td>0.942674</td>\n",
              "      <td>1.032048</td>\n",
              "      <td>1.171602</td>\n",
              "      <td>1.033197</td>\n",
              "      <td>1.233479</td>\n",
              "      <td>1.100245</td>\n",
              "      <td>1.272283</td>\n",
              "      <td>0.723518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229259</td>\n",
              "      <td>0.229644</td>\n",
              "      <td>0.488496</td>\n",
              "      <td>0.267395</td>\n",
              "      <td>2.466339</td>\n",
              "      <td>0.465563</td>\n",
              "      <td>0.313102</td>\n",
              "      <td>0.405502</td>\n",
              "      <td>0.030188</td>\n",
              "      <td>0.030188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.739548e+07</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.529711e+07</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.712102e+07</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.910558e+07</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.796186e+07</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           CD_ALUNO             Q1             Q2             Q3  \\\n",
              "count  1.205960e+05  120596.000000  120596.000000  120596.000000   \n",
              "mean   2.734087e+07       1.803567       4.183397       4.234850   \n",
              "std    2.946464e+06       0.634012       0.942674       1.032048   \n",
              "min    1.739548e+07       1.000000       1.000000       1.000000   \n",
              "25%    2.529711e+07       1.000000       4.000000       4.000000   \n",
              "50%    2.712102e+07       2.000000       4.000000       5.000000   \n",
              "75%    2.910558e+07       2.000000       5.000000       5.000000   \n",
              "max    3.796186e+07       4.000000       5.000000       5.000000   \n",
              "\n",
              "                  Q4             Q5             Q6             Q7  \\\n",
              "count  120596.000000  120596.000000  120596.000000  120596.000000   \n",
              "mean        3.901249       3.994676       3.839248       4.015813   \n",
              "std         1.171602       1.033197       1.233479       1.100245   \n",
              "min         1.000000       1.000000       1.000000       1.000000   \n",
              "25%         3.000000       3.000000       3.000000       3.000000   \n",
              "50%         4.000000       4.000000       4.000000       4.000000   \n",
              "75%         5.000000       5.000000       5.000000       5.000000   \n",
              "max         5.000000       5.000000       5.000000       5.000000   \n",
              "\n",
              "                  Q8             Q9  ...  \\\n",
              "count  120596.000000  120596.000000  ...   \n",
              "mean        3.542456       4.551519  ...   \n",
              "std         1.272283       0.723518  ...   \n",
              "min         1.000000       3.000000  ...   \n",
              "25%         3.000000       4.000000  ...   \n",
              "50%         4.000000       5.000000  ...   \n",
              "75%         5.000000       5.000000  ...   \n",
              "max         5.000000       5.000000  ...   \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto  \\\n",
              "count                                      120596.000000            \n",
              "mean                                            0.055657            \n",
              "std                                             0.229259            \n",
              "min                                             0.000000            \n",
              "25%                                             0.000000            \n",
              "50%                                             0.000000            \n",
              "75%                                             0.000000            \n",
              "max                                             1.000000            \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana de Sorocaba  \\\n",
              "count                                      120596.000000      \n",
              "mean                                            0.055856      \n",
              "std                                             0.229644      \n",
              "min                                             0.000000      \n",
              "25%                                             0.000000      \n",
              "50%                                             0.000000      \n",
              "75%                                             0.000000      \n",
              "max                                             1.000000      \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana de São Paulo  \\\n",
              "count                                      120596.000000       \n",
              "mean                                            0.393355       \n",
              "std                                             0.488496       \n",
              "min                                             0.000000       \n",
              "25%                                             0.000000       \n",
              "50%                                             0.000000       \n",
              "75%                                             1.000000       \n",
              "max                                             1.000000       \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte  \\\n",
              "count                                      120596.000000                             \n",
              "mean                                            0.077507                             \n",
              "std                                             0.267395                             \n",
              "min                                             0.000000                             \n",
              "25%                                             0.000000                             \n",
              "50%                                             0.000000                             \n",
              "75%                                             0.000000                             \n",
              "max                                             1.000000                             \n",
              "\n",
              "               idade  PERIODO_MANHÃ  PERIODO_NOITE  PERIODO_TARDE  \\\n",
              "count  120596.000000  120596.000000  120596.000000  120596.000000   \n",
              "mean       15.735464       0.682353       0.110169       0.207478   \n",
              "std         2.466339       0.465563       0.313102       0.405502   \n",
              "min        10.000000       0.000000       0.000000       0.000000   \n",
              "25%        15.000000       0.000000       0.000000       0.000000   \n",
              "50%        16.000000       1.000000       0.000000       0.000000   \n",
              "75%        18.000000       1.000000       0.000000       0.000000   \n",
              "max        56.000000       1.000000       1.000000       1.000000   \n",
              "\n",
              "        Tipo_PROVA_A   Tipo_PROVA_C  \n",
              "count  120596.000000  120596.000000  \n",
              "mean        0.000912       0.999088  \n",
              "std         0.030188       0.030188  \n",
              "min         0.000000       0.000000  \n",
              "25%         0.000000       1.000000  \n",
              "50%         0.000000       1.000000  \n",
              "75%         0.000000       1.000000  \n",
              "max         1.000000       1.000000  \n",
              "\n",
              "[8 rows x 89 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120596, 89)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outliers detection\n",
        "\n",
        "Since the noly original numerical columns were the porc_ACERT, we will only perform outlier detection on these. The method to find the outliers will be though z-score and standard deviation, it's known that 99% of data is located between -3 std to 3 std, with that in mind we will drop all the data the is beyond this range  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00024876446979999335\n",
            "0.0\n",
            "0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD5CAYAAADcDXXiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARmElEQVR4nO3de5BkZ13G8e9DEuUaTCpkTGXRCRrBGI3KVMBL4aYWBIQysSSSLWWXFNaKouIFdaEs8VKUa+GNKlFcQdhgBGLESjQawXVbQeWy4Z4sIUhiWFizoAJujJLIzz/mLLTLTGamT/d2z9vfT9XU9Hn7XH7d78zTZ945l1QVkqS2PGDaBUiSxs9wl6QGGe6S1CDDXZIaZLhLUoNOnXYBAGeddVYtLi5Ou4yJufvuu3nIQx4y7TI0Ivtv82q972666aZPVtUjVnpuJsJ9cXGRgwcPTruMiRkMBmzdunXaZWhE9t/m1XrfJfmX1Z5zWEaSGmS4S1KDDHdJapDhLkkNMtwlqUFrhnuSP0xyNMkHhtrOTPLmJLd1388Yeu6FST6c5NYkT55U4ZKk1a1nz/01wFNOaNsN7K+q84H93TRJLgCuAL6uW+Z3k5wytmolSeuyZrhX1d8D/35C86XAvu7xPuCyofbXV9X/VNXtwIeBi8dTqiRpvUY9iWmhqo4AVNWRJGd37ecCbxua73DX9kWS7AJ2ASwsLDAYDEYsZfouueSS3us4cODAGCrRRo2j78D+mxb7b3XjPkM1K7SteDeQqtoL7AVYWlqqzXwW2Vo3PFncfQN37HnaSapGG7Gem9XYf7PL373VjXq0zF1JzgHovh/t2g8Djxyabwvw8dHLkySNYtRwvx7Y2T3eCVw31H5Fki9Nch5wPvCOfiVKkjZqzWGZJK8DtgJnJTkMvBjYA1yT5DnAncDlAFV1c5JrgFuA+4DnVdX/Tqh2SdIq1gz3qtq+ylPbVpn/JcBL+hQlSerHM1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWpQr3BP8pNJbk7ygSSvS/LAJGcmeXOS27rvZ4yrWEnS+owc7knOBX4cWKqqC4FTgCuA3cD+qjof2N9NS5JOor7DMqcCD0pyKvBg4OPApcC+7vl9wGU9tyFJ2qBTR12wqj6W5NeBO4F7gDdV1ZuSLFTVkW6eI0nOXmn5JLuAXQALCwsMBoNRS9kUWn99rbP/Nq957buRw70bS78UOA/4FPAnSX5gvctX1V5gL8DS0lJt3bp11FIm6qJfehOfvufe3ut59o1391r+4Q86jfe++Dt716ER3HgDs/rzqTXMcd+NHO7AE4Hbq+oTAEneCHwrcFeSc7q99nOAo2Ooc2o+fc+93LHnab3WMRgMev+ALe6+odfykuZLnzH3O4HHJ3lwkgDbgEPA9cDObp6dwHX9SpQkbVSfMfe3J7kWeBdwH/BulodZHgpck+Q5LH8AXD6OQiVJ69dnWIaqejHw4hOa/4flvXhJ0pR4hqokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGtTrwmGSNEnjuFlO33shbNYb5RjukmZW35vlzPONchyWkaQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3yUEg1bRzHSUO/w+E263HS2twMdzWt73HS0P9Y6c16nLQ2N4dlJKlBhrskNchwl6QGGe6S1CDDXZIaZLhLUoM8FHIND/va3Xz9vt39V7Svbx0A/Q7pkzQ/DPc1/OehPVM/Tho8VlrSxvQalknyZUmuTfLBJIeSfEuSM5O8Oclt3fczxlWsJGl9+o65vwy4saoeA1wEHAJ2A/ur6nxgfzctSTqJRg73JKcDTwBeBVBVn62qTwGX8oUR5n3AZf1KlCRtVJ8x90cBnwBeneQi4Cbg+cBCVR0BqKojSc5eaeEku4BdAAsLCwwGgx6lTFbf2o4dOzaW1zfL79Esm4X+s+9G1+e9m+vfvaoa6QtYAu4DHtdNvwz4FeBTJ8z3H2ut67GPfWzNqq/8ub/ovY4DBw7MRB3zaBb6z74bXd/3rvXfPeBgrZKrfcbcDwOHq+rt3fS1wDcDdyU5B6D7frTHNiRJIxg53KvqX4GPJnl017QNuAW4HtjZte0ErutVoSRpw/oe5/5jwNVJvgT4CHAlyx8Y1yR5DnAncHnPbUiaU2M5iXBOTyDsFe5V9R6Wx95PtK3PeiUJ+p9EOM8nEHptGUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoL7Xc5dm2liuBw69rgm+Wa8Hrs3NcFfT+l4PHPpfE3yzXg9cm5vDMpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBHgopaab1PpT0xn7LP/xBp/Xb/pQY7pJmVt9zFBZ339B7HZuVwzKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWo93HuSU4BDgIfq6qnJzkTeAOwCNwBfF9V/Uff7UzTWK7HPacnUkiajnGcxPR84BBweje9G9hfVXuS7O6mf24M25mKcZwAMc8nUkiajl7DMkm2sHz/sFcONV/KF25Ktg+4rM82JEkb13fP/beBnwUeNtS2UFVHAKrqSJKzV1owyS5gF8DCwgKDwaBnKbOt9dc3y/q+98eOHeu9Dvt/eub1vR853JM8HThaVTcl2brR5atqL7AXYGlpqfrco3Lm3XhDr3twqocxvPd976Fq/0/RHL/3ffbcvw347iTfBTwQOD3JHwF3JTmn22s/Bzg6jkIlSes38ph7Vb2wqrZU1SJwBfC3VfUDwPXAzm62ncB1vauUJG3IJI5z3wM8KcltwJO6aUnSSTSW67lX1QAYdI//Ddg2jvVK4zDt8xQ8R0HT4M061DTPU9C88vIDktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGjnckzwyyYEkh5LcnOT5XfuZSd6c5Lbu+xnjK1eStB599tzvA366qr4WeDzwvCQXALuB/VV1PrC/m5YknUQjh3tVHamqd3WP/xM4BJwLXArs62bbB1zWs0ZJ0gadOo6VJFkEvgl4O7BQVUdg+QMgydmrLLML2AWwsLDAYDAYRykzq/XX1zr7b/Oa177rHe5JHgr8KfATVfWZJOtarqr2AnsBlpaWauvWrX1LmV033kDTr6919t/mNcd91+tomSSnsRzsV1fVG7vmu5Kc0z1/DnC0X4mSpI3qc7RMgFcBh6rqN4eeuh7Y2T3eCVw3enmSpFH0GZb5NuBZwPuTvKdrexGwB7gmyXOAO4HLe1UoSdqwkcO9qt4KrDbAvm3U9UqS+vMMVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDepzD1VJmqpktTt9Ds3za2uvp6rGUM1scc9d0qZVVff7deDAgTXnaTHYwXCXpCYZ7pLUIMNdkhrkP1QlNWelf7S2Ora+GvfcJTVlONi3bNmyYvs8MNwlNamqeO1rXzt3e+zHGe6SmnPBBRfc7/Q8MNwlNeeWW2653+l54D9UJTUpCVu2bOHw4cPTLmUq3HOX1JThMfbhYJ+3sXfDXVJzVrr8wLyZ2LBMkqcALwNOAV5ZVXsmtS1JGuZx7hPac09yCvBy4KnABcD2JPP372pJJ91wsO/YsWPF9nkwqWGZi4EPV9VHquqzwOuBSye0LUn6IlXFlVdeOXd77MdNaljmXOCjQ9OHgccNz5BkF7ALYGFhgcFgMKFSJu+SSy5Zc561ril94MCBMVWjjVhP34H9t9ns2LGDwWDAsWPHGAwG7Nixg6uuumpT58xGZRKfakkuB55cVT/YTT8LuLiqfmyl+ZeWlurgwYNjr2NWDAYDtm7dOu0yNCL7b3M5PvxSVZ/vu+G2liS5qaqWVnpuUsMyh4FHDk1vAT4+oW1J0hdJwqtf/eq5G2s/blLh/k7g/CTnJfkS4Arg+gltS5I+b3jv/KqrrlqxfR5MJNyr6j7gR4G/Bg4B11TVzZPYliSdyOPcJ3ice1X9JfCXk1q/JGl1nqEqSQ0y3CWpQYa7JDXIcJekBk3kJKYNF5F8AviXadcxQWcBn5x2ERqZ/bd5td53X1lVj1jpiZkI99YlObjaWWSaffbf5jXPfeewjCQ1yHCXpAYZ7ifH3mkXoF7sv81rbvvOMXdJapB77pLUIMNdkhpkuEtSgwz3MUlyXZJ/WqH9BUk+mOQDSd6bZEfXPkhya5L3dF/Xdu2/mORjXdstSbYnuXJovs8meX/3eM8qtTw7ye9M9hVvPjPYR5Vk21Db93Rtzxhqe0SSe5P8UDf98qHt3jO0zWestJ0WzVI/dut5apKDSQ512//1ofW/oHv8miS3D637Hyfz7gw5fq1jv77wBZyywfm/jOV7xh4Czhtqfy7L17Q/vZt+OLCzezwAllZY1y8CL+genw98Bjht6Pk7gLPWqOfZwO9M+320j9bso/cBrxxqewPwHuAZQ20/ArwFGJyw/CLwgWn3g/3IhcA/A4/ppk8FfmSF9b9muF9Pxleze+5JFrtP0X1J3pfk2iQPTrItybu7T+Q/TPKl3fx3JPmFJG8FLk/ylCTv6vYA9q+xue8F/hx4Pct3nTruRSx39GcAqurTVbVvva+hqm4D/gs4YwMv/f/p9hhekeQtST6U5Omjrmvc7CPeAlyc5LQkDwW+muVwH7Yd+GlgS5JzR9jGxM15P/4s8JKq+mC3nvuq6nc3uI6JaDbcO48G9lbVN7D8qfxTLH+CPrOqvp7lT9kfHpr/v6vq24H9wB8A31tVFwGXr7Gd7cDruq/tAEkeBjysqv75fpa7eujPtJee+GSSbwZuq6qja7/U+7UIfAfwNOAVSR7Yc33jNM99VMDfAE8GLuWEW1EmeSTw5VX1DuAa4JkjbONkmdd+vBC4aZ3zvnSohqs3uJ0Naz3cP1pV/9A9/iNgG3B7VX2oa9sHPGFo/jd03x8P/H1V3Q5QVf++2gaSLLC8x/XWbr33JbkQCMu/vPfn+6vqG7uvnxlq/8kktwJvZ/lPu76uqarPdXsnHwEeM4Z1jsu899HxPdArWA6sYVewHOrH59veYzuTNu/9uB4/M1TD9094W82H+0bP0Lq7+76eH5bjnsnyn3K3J7mD5b3kK7o/D+9O8qgN1gDwW1X16G7dV41hT/vE1zJLZ67NdR91e+UXsjy2+6ETnt4OPLur+XrgoiTnj7Kdk2Be+/Fm4LEjbHfiWg/3r0jyLd3j7Sz/CbyY5Ku7tmcBf7fCcv8EfEeS8wCSnHk/29gOPKWqFqtqkeWOPj4W+KvAy5Oc3q3n9CS71lt8Vb0ROAjsXO8yq7g8yQOSfBXwKODWnusbJ/sIXsjymPHnJXk08JCqOneo7l/l/48zz5J57ceXAi9K8jXddh+Q5Kc2uI6JmNgNsmfEIWBnkt8HbgOeD7wN+JMkpwLvBF5x4kJV9YnuB+ONSR4AHAWedOJ8SRaBr+jWeXzZ25N8JsnjgN8DHgq8M8m9wL3Abwyt4uok93SPP1lVT1zhNfwy8MdJ/qCqPrexl/95t7L8i7UAPLeq/nvE9UzC3PdRVf3VCs3bgT87oe1PWR6e+ZWNbuMkmMt+rKr3JfkJ4HVJHszyXyE3rDL7S5P8/ND0xVX12fVsZxTNXlum+2H4i6q6cNq1TFOS17D8Plw77VpOZB+1wX6cTa0Py0jSXGp2z33cklzJ8p+aw/6hqp43jXpgNmuapll8P2axplk3i+/ZLNa0FsNdkhrksIwkNchwl6QGGe6S1CDDXZIa9H/qhZKJoMut5gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#removing outliers\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "print(len(df[(np.abs(stats.zscore(df['porc_ACERT_CIE'])) > 3)])/len(df.index)) #less than 0.025%\n",
        "print(len(df[(np.abs(stats.zscore(df['porc_ACERT_MAT'])) > 3)])/len(df.index))\n",
        "print(len(df[(np.abs(stats.zscore(df['porc_ACERT_lp'])) > 3)])/len(df.index))\n",
        "\n",
        "df.boxplot(column=['porc_ACERT_lp',\n",
        "       'porc_ACERT_MAT', 'porc_ACERT_CIE'])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there are less than 0.025% of data that are outliers, there will be no great negative effects in dropping it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[(np.abs(stats.zscore(df['porc_ACERT_CIE'])) < 3)]\n",
        "type(df)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing df's for each specific target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cie = df \n",
        "df_mat = df\n",
        "df_lp = df\n",
        "\n",
        "\n",
        "# df_cie = df.drop(columns=['porc_ACERT_lp', 'porc_ACERT_MAT'])\n",
        "\n",
        "# df_mat = df.drop(columns=['porc_ACERT_lp', 'porc_ACERT_CIE'])\n",
        "\n",
        "# df_lp = df.drop(columns=['porc_ACERT_CIE', 'porc_ACERT_MAT'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10',\n",
              "       'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20',\n",
              "       'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30',\n",
              "       'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40',\n",
              "       'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50',\n",
              "       'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60',\n",
              "       'Q61', 'Q62', 'SERIE_ANO', 'TP_SEXO', 'Tem_Nec', 'porc_ACERT_lp',\n",
              "       'porc_ACERT_MAT', 'porc_ACERT_CIE', 'nivel_profic_lp',\n",
              "       'nivel_profic_mat', 'nivel_profic_cie', 'Q63_A', 'Q63_B', 'Q63_C',\n",
              "       'Q63_D', 'RegiaoMetropolitana_Interior',\n",
              "       'RegiaoMetropolitana_Região Metropolitana da Baixada Santista',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Campinas',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Sorocaba',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de São Paulo',\n",
              "       'RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte',\n",
              "       'idade', 'PERIODO_MANHÃ', 'PERIODO_NOITE', 'PERIODO_TARDE',\n",
              "       'Tipo_PROVA_A', 'Tipo_PROVA_C'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cie.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def tranform_into_lin_reg(df):\n",
        "    \"\"\"Drop all the categorical columns in the dataframe lin_reg_df\"\"\"\n",
        "    df = df.drop(columns=['nivel_profic_mat', 'nivel_profic_lp', 'nivel_profic_cie'])\n",
        "    return df\n",
        "df_cie_lr = tranform_into_lin_reg(df_cie)\n",
        "print(get_categorical_columns(df_cie_lr))\n",
        "df_mat_lr = tranform_into_lin_reg(df_mat)\n",
        "print(get_categorical_columns(df_mat_lr))\n",
        "df_lp_lr  = tranform_into_lin_reg(df_lp)\n",
        "print(get_categorical_columns(df_lp_lr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CD_ALUNO', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10',\n",
              "       'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20',\n",
              "       'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30',\n",
              "       'Q31', 'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40',\n",
              "       'Q41', 'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50',\n",
              "       'Q51', 'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60',\n",
              "       'Q61', 'Q62', 'SERIE_ANO', 'TP_SEXO', 'Tem_Nec', 'porc_ACERT_lp',\n",
              "       'porc_ACERT_MAT', 'porc_ACERT_CIE', 'Q63_A', 'Q63_B', 'Q63_C', 'Q63_D',\n",
              "       'RegiaoMetropolitana_Interior',\n",
              "       'RegiaoMetropolitana_Região Metropolitana da Baixada Santista',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Campinas',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Sorocaba',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de São Paulo',\n",
              "       'RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte',\n",
              "       'idade', 'PERIODO_MANHÃ', 'PERIODO_NOITE', 'PERIODO_TARDE',\n",
              "       'Tipo_PROVA_A', 'Tipo_PROVA_C'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cie_lr.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop CD_ALUNO\n",
        "df_cie_lr = df_cie_lr.drop(columns=['CD_ALUNO'])\n",
        "df_mat_lr = df_mat_lr.drop(columns=['CD_ALUNO'])\n",
        "df_lp_lr = df_lp_lr.drop(columns=['CD_ALUNO'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
              "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
              "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
              "       'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40', 'Q41',\n",
              "       'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50', 'Q51',\n",
              "       'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60', 'Q61',\n",
              "       'Q62', 'SERIE_ANO', 'TP_SEXO', 'Tem_Nec', 'porc_ACERT_lp',\n",
              "       'porc_ACERT_MAT', 'porc_ACERT_CIE', 'Q63_A', 'Q63_B', 'Q63_C', 'Q63_D',\n",
              "       'RegiaoMetropolitana_Interior',\n",
              "       'RegiaoMetropolitana_Região Metropolitana da Baixada Santista',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Campinas',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Sorocaba',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de São Paulo',\n",
              "       'RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte',\n",
              "       'idade', 'PERIODO_MANHÃ', 'PERIODO_NOITE', 'PERIODO_TARDE',\n",
              "       'Tipo_PROVA_A', 'Tipo_PROVA_C'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cie_lr.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to normalize the data\"\"\"\n",
        "def normalize_data(df):\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df)\n",
        "    return scaler.transform(df)\n",
        "df_cie_lr = pd.DataFrame(normalize_data(df_cie_lr), columns=df_cie_lr.columns,index=df_cie_lr.index)\n",
        "df_mat_lr = pd.DataFrame(normalize_data(df_mat_lr), columns=df_mat_lr.columns,index=df_mat_lr.index)\n",
        "df_lp_lr = pd.DataFrame(normalize_data(df_lp_lr), columns=df_lp_lr.columns,index=df_lp_lr.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q1</th>\n",
              "      <th>Q2</th>\n",
              "      <th>Q3</th>\n",
              "      <th>Q4</th>\n",
              "      <th>Q5</th>\n",
              "      <th>Q6</th>\n",
              "      <th>Q7</th>\n",
              "      <th>Q8</th>\n",
              "      <th>Q9</th>\n",
              "      <th>Q10</th>\n",
              "      <th>...</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Sorocaba</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de São Paulo</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte</th>\n",
              "      <th>idade</th>\n",
              "      <th>PERIODO_MANHÃ</th>\n",
              "      <th>PERIODO_NOITE</th>\n",
              "      <th>PERIODO_TARDE</th>\n",
              "      <th>Tipo_PROVA_A</th>\n",
              "      <th>Tipo_PROVA_C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "      <td>1.205660e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.049024e-16</td>\n",
              "      <td>-4.315142e-16</td>\n",
              "      <td>-1.772732e-16</td>\n",
              "      <td>1.222584e-16</td>\n",
              "      <td>-7.637836e-17</td>\n",
              "      <td>-1.132710e-16</td>\n",
              "      <td>7.331380e-17</td>\n",
              "      <td>5.504428e-17</td>\n",
              "      <td>2.946696e-18</td>\n",
              "      <td>4.537912e-16</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.793608e-17</td>\n",
              "      <td>4.019294e-17</td>\n",
              "      <td>1.379054e-17</td>\n",
              "      <td>2.805255e-17</td>\n",
              "      <td>-1.446238e-16</td>\n",
              "      <td>-1.155105e-17</td>\n",
              "      <td>3.500675e-17</td>\n",
              "      <td>-1.183393e-16</td>\n",
              "      <td>-1.284760e-17</td>\n",
              "      <td>1.941283e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "      <td>1.000004e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.267496e+00</td>\n",
              "      <td>-3.377497e+00</td>\n",
              "      <td>-3.135009e+00</td>\n",
              "      <td>-2.476635e+00</td>\n",
              "      <td>-2.898725e+00</td>\n",
              "      <td>-2.301823e+00</td>\n",
              "      <td>-2.741345e+00</td>\n",
              "      <td>-1.998341e+00</td>\n",
              "      <td>-2.144420e+00</td>\n",
              "      <td>-4.759523e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.427251e-01</td>\n",
              "      <td>-2.432228e-01</td>\n",
              "      <td>-8.052361e-01</td>\n",
              "      <td>-2.898989e-01</td>\n",
              "      <td>-2.325815e+00</td>\n",
              "      <td>-1.465640e+00</td>\n",
              "      <td>-3.518703e-01</td>\n",
              "      <td>-5.116614e-01</td>\n",
              "      <td>-3.021914e-02</td>\n",
              "      <td>-3.309161e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.267496e+00</td>\n",
              "      <td>-1.946253e-01</td>\n",
              "      <td>-2.276916e-01</td>\n",
              "      <td>-7.694097e-01</td>\n",
              "      <td>-9.628269e-01</td>\n",
              "      <td>-6.803548e-01</td>\n",
              "      <td>-9.233868e-01</td>\n",
              "      <td>-4.262935e-01</td>\n",
              "      <td>-7.622820e-01</td>\n",
              "      <td>3.296665e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.427251e-01</td>\n",
              "      <td>-2.432228e-01</td>\n",
              "      <td>-8.052361e-01</td>\n",
              "      <td>-2.898989e-01</td>\n",
              "      <td>-2.982680e-01</td>\n",
              "      <td>-1.465640e+00</td>\n",
              "      <td>-3.518703e-01</td>\n",
              "      <td>-5.116614e-01</td>\n",
              "      <td>-3.021914e-02</td>\n",
              "      <td>3.021914e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.099038e-01</td>\n",
              "      <td>-1.946253e-01</td>\n",
              "      <td>7.414143e-01</td>\n",
              "      <td>8.420298e-02</td>\n",
              "      <td>5.122103e-03</td>\n",
              "      <td>1.303794e-01</td>\n",
              "      <td>-1.440754e-02</td>\n",
              "      <td>3.597300e-01</td>\n",
              "      <td>6.198564e-01</td>\n",
              "      <td>3.296665e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.427251e-01</td>\n",
              "      <td>-2.432228e-01</td>\n",
              "      <td>-8.052361e-01</td>\n",
              "      <td>-2.898989e-01</td>\n",
              "      <td>1.072414e-01</td>\n",
              "      <td>6.822956e-01</td>\n",
              "      <td>-3.518703e-01</td>\n",
              "      <td>-5.116614e-01</td>\n",
              "      <td>-3.021914e-02</td>\n",
              "      <td>3.021914e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.099038e-01</td>\n",
              "      <td>8.663319e-01</td>\n",
              "      <td>7.414143e-01</td>\n",
              "      <td>9.378157e-01</td>\n",
              "      <td>9.730711e-01</td>\n",
              "      <td>9.411137e-01</td>\n",
              "      <td>8.945717e-01</td>\n",
              "      <td>1.145754e+00</td>\n",
              "      <td>6.198564e-01</td>\n",
              "      <td>3.296665e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.427251e-01</td>\n",
              "      <td>-2.432228e-01</td>\n",
              "      <td>1.241872e+00</td>\n",
              "      <td>-2.898989e-01</td>\n",
              "      <td>9.182604e-01</td>\n",
              "      <td>6.822956e-01</td>\n",
              "      <td>-3.518703e-01</td>\n",
              "      <td>-5.116614e-01</td>\n",
              "      <td>-3.021914e-02</td>\n",
              "      <td>3.021914e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.464702e+00</td>\n",
              "      <td>8.663319e-01</td>\n",
              "      <td>7.414143e-01</td>\n",
              "      <td>9.378157e-01</td>\n",
              "      <td>9.730711e-01</td>\n",
              "      <td>9.411137e-01</td>\n",
              "      <td>8.945717e-01</td>\n",
              "      <td>1.145754e+00</td>\n",
              "      <td>6.198564e-01</td>\n",
              "      <td>3.296665e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>4.119886e+00</td>\n",
              "      <td>4.111456e+00</td>\n",
              "      <td>1.241872e+00</td>\n",
              "      <td>3.449478e+00</td>\n",
              "      <td>1.632762e+01</td>\n",
              "      <td>6.822956e-01</td>\n",
              "      <td>2.841956e+00</td>\n",
              "      <td>1.954418e+00</td>\n",
              "      <td>3.309161e+01</td>\n",
              "      <td>3.021914e-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 85 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q1            Q2            Q3            Q4            Q5  \\\n",
              "count  1.205660e+05  1.205660e+05  1.205660e+05  1.205660e+05  1.205660e+05   \n",
              "mean   1.049024e-16 -4.315142e-16 -1.772732e-16  1.222584e-16 -7.637836e-17   \n",
              "std    1.000004e+00  1.000004e+00  1.000004e+00  1.000004e+00  1.000004e+00   \n",
              "min   -1.267496e+00 -3.377497e+00 -3.135009e+00 -2.476635e+00 -2.898725e+00   \n",
              "25%   -1.267496e+00 -1.946253e-01 -2.276916e-01 -7.694097e-01 -9.628269e-01   \n",
              "50%    3.099038e-01 -1.946253e-01  7.414143e-01  8.420298e-02  5.122103e-03   \n",
              "75%    3.099038e-01  8.663319e-01  7.414143e-01  9.378157e-01  9.730711e-01   \n",
              "max    3.464702e+00  8.663319e-01  7.414143e-01  9.378157e-01  9.730711e-01   \n",
              "\n",
              "                 Q6            Q7            Q8            Q9           Q10  \\\n",
              "count  1.205660e+05  1.205660e+05  1.205660e+05  1.205660e+05  1.205660e+05   \n",
              "mean  -1.132710e-16  7.331380e-17  5.504428e-17  2.946696e-18  4.537912e-16   \n",
              "std    1.000004e+00  1.000004e+00  1.000004e+00  1.000004e+00  1.000004e+00   \n",
              "min   -2.301823e+00 -2.741345e+00 -1.998341e+00 -2.144420e+00 -4.759523e+00   \n",
              "25%   -6.803548e-01 -9.233868e-01 -4.262935e-01 -7.622820e-01  3.296665e-01   \n",
              "50%    1.303794e-01 -1.440754e-02  3.597300e-01  6.198564e-01  3.296665e-01   \n",
              "75%    9.411137e-01  8.945717e-01  1.145754e+00  6.198564e-01  3.296665e-01   \n",
              "max    9.411137e-01  8.945717e-01  1.145754e+00  6.198564e-01  3.296665e-01   \n",
              "\n",
              "       ...  RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto  \\\n",
              "count  ...                                       1.205660e+05            \n",
              "mean   ...                                      -6.793608e-17            \n",
              "std    ...                                       1.000004e+00            \n",
              "min    ...                                      -2.427251e-01            \n",
              "25%    ...                                      -2.427251e-01            \n",
              "50%    ...                                      -2.427251e-01            \n",
              "75%    ...                                      -2.427251e-01            \n",
              "max    ...                                       4.119886e+00            \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana de Sorocaba  \\\n",
              "count                                       1.205660e+05      \n",
              "mean                                        4.019294e-17      \n",
              "std                                         1.000004e+00      \n",
              "min                                        -2.432228e-01      \n",
              "25%                                        -2.432228e-01      \n",
              "50%                                        -2.432228e-01      \n",
              "75%                                        -2.432228e-01      \n",
              "max                                         4.111456e+00      \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana de São Paulo  \\\n",
              "count                                       1.205660e+05       \n",
              "mean                                        1.379054e-17       \n",
              "std                                         1.000004e+00       \n",
              "min                                        -8.052361e-01       \n",
              "25%                                        -8.052361e-01       \n",
              "50%                                        -8.052361e-01       \n",
              "75%                                         1.241872e+00       \n",
              "max                                         1.241872e+00       \n",
              "\n",
              "       RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte  \\\n",
              "count                                       1.205660e+05                             \n",
              "mean                                        2.805255e-17                             \n",
              "std                                         1.000004e+00                             \n",
              "min                                        -2.898989e-01                             \n",
              "25%                                        -2.898989e-01                             \n",
              "50%                                        -2.898989e-01                             \n",
              "75%                                        -2.898989e-01                             \n",
              "max                                         3.449478e+00                             \n",
              "\n",
              "              idade  PERIODO_MANHÃ  PERIODO_NOITE  PERIODO_TARDE  \\\n",
              "count  1.205660e+05   1.205660e+05   1.205660e+05   1.205660e+05   \n",
              "mean  -1.446238e-16  -1.155105e-17   3.500675e-17  -1.183393e-16   \n",
              "std    1.000004e+00   1.000004e+00   1.000004e+00   1.000004e+00   \n",
              "min   -2.325815e+00  -1.465640e+00  -3.518703e-01  -5.116614e-01   \n",
              "25%   -2.982680e-01  -1.465640e+00  -3.518703e-01  -5.116614e-01   \n",
              "50%    1.072414e-01   6.822956e-01  -3.518703e-01  -5.116614e-01   \n",
              "75%    9.182604e-01   6.822956e-01  -3.518703e-01  -5.116614e-01   \n",
              "max    1.632762e+01   6.822956e-01   2.841956e+00   1.954418e+00   \n",
              "\n",
              "       Tipo_PROVA_A  Tipo_PROVA_C  \n",
              "count  1.205660e+05  1.205660e+05  \n",
              "mean  -1.284760e-17  1.941283e-16  \n",
              "std    1.000004e+00  1.000004e+00  \n",
              "min   -3.021914e-02 -3.309161e+01  \n",
              "25%   -3.021914e-02  3.021914e-02  \n",
              "50%   -3.021914e-02  3.021914e-02  \n",
              "75%   -3.021914e-02  3.021914e-02  \n",
              "max    3.309161e+01  3.021914e-02  \n",
              "\n",
              "[8 rows x 85 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cie_lr.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to split the data into train and validation sets\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "def split_data(df, target, validation_size):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return train_test_split(X, y, test_size=validation_size, random_state=42)\n",
        "\n",
        "    \n",
        "X_train_cie, X_val_cie, y_train_cie, y_val_cie = split_data(df_cie_lr, 'porc_ACERT_CIE', 0.2)\n",
        "X_train_mat, X_val_mat, y_train_mat, y_val_mat = split_data(df_mat_lr, 'porc_ACERT_MAT', 0.2)\n",
        "X_train_lp, X_val_lp, y_train_lp, y_val_lp = split_data(df_lp_lr, 'porc_ACERT_lp', 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 0 ns\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\"\"\"Function to evaluate the model using the validation set and the R2 score\"\"\"\n",
        "def evaluate_model(X_val, y_val, w, b):\n",
        "    from sklearn.metrics import r2_score\n",
        "    y_pred = np.dot(X_val, w) + b\n",
        "    error = y_pred - y_val\n",
        "    mse = (1/X_val.shape[0]) * np.sum(error**2)\n",
        "    return mse, r2_score(y_val, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "D9cpdif9JxFR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 0 ns\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# TODO: Linear Regression. Implement your solution. You cannot use scikit-learn, Keras/TensorFlow, or PyTorch libraries.\n",
        "\n",
        "\n",
        "def linear_regression(X_train, y_train, X_val, y_val, learning_rate, epochs):\n",
        "    train_loss, val_loss,  r2_list = [], [], []\n",
        "    w = np.random.rand(X_train.shape[1])*0.001 - 0.0005\n",
        "    b = 0\n",
        "    m = X_train.shape[0]\n",
        "    for i in range(epochs):\n",
        "        y_pred = np.dot(X_train, w) + b\n",
        "        error = y_pred - y_train\n",
        "        mse = (1/m) * np.sum(error**2)\n",
        "        w = w - (learning_rate * (1/m) * np.dot(X_train.T, error))\n",
        "        b = b - (learning_rate * (1/m) * np.sum(error))\n",
        "        mse_val, r2 = evaluate_model(X_val, y_val, w, b)\n",
        "        train_loss.append(mse)\n",
        "        val_loss.append(mse_val)\n",
        "        r2_list.append(r2)\n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch: {}, Train Loss: {}, Val Loss: {}, R2: {}\".format(\n",
        "                i, mse, mse_val, r2))\n",
        "    return w, b, train_loss, val_loss, r2_list\n",
        "\n",
        "\n",
        "#print('Science')\n",
        "#w_cie, b_cie, train_loss_cie, val_loss_cie, r2_cie = linear_regression(\n",
        "#    X_train_cie, y_train_cie, X_val_cie, y_val_cie, 0.01, 1000)\n",
        "#print('Math')\n",
        "#w_mat, b_mat,  train_loss_mat, val_loss_mat, r2_mat = linear_regression(\n",
        "#    X_train_mat, y_train_mat, X_val_mat, y_val_mat, 0.01, 1000)\n",
        "#print('Portuguese')\n",
        "#w_lp, b_lp,  train_loss_lp, val_loss_lp, r2_lp = linear_regression(\n",
        "#    X_train_lp, y_train_lp, X_val_lp, y_val_lp, 0.01, 1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q1</th>\n",
              "      <th>Q2</th>\n",
              "      <th>Q3</th>\n",
              "      <th>Q4</th>\n",
              "      <th>Q5</th>\n",
              "      <th>Q6</th>\n",
              "      <th>Q7</th>\n",
              "      <th>Q8</th>\n",
              "      <th>Q9</th>\n",
              "      <th>Q10</th>\n",
              "      <th>...</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de Sorocaba</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana de São Paulo</th>\n",
              "      <th>RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte</th>\n",
              "      <th>idade</th>\n",
              "      <th>PERIODO_MANHÃ</th>\n",
              "      <th>PERIODO_NOITE</th>\n",
              "      <th>PERIODO_TARDE</th>\n",
              "      <th>Tipo_PROVA_A</th>\n",
              "      <th>Tipo_PROVA_C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>0.866332</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>0.937816</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>0.894572</td>\n",
              "      <td>1.145754</td>\n",
              "      <td>-0.762282</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>3.449478</td>\n",
              "      <td>0.918260</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>-0.194625</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>-0.769410</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>0.894572</td>\n",
              "      <td>1.145754</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>1.241872</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>-1.514796</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>0.866332</td>\n",
              "      <td>-2.165903</td>\n",
              "      <td>0.084203</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>-1.491089</td>\n",
              "      <td>-0.014408</td>\n",
              "      <td>-0.426294</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>-0.298268</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>-0.194625</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>0.084203</td>\n",
              "      <td>-0.962827</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>-0.014408</td>\n",
              "      <td>0.359730</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>1.241872</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>0.918260</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>-0.194625</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>0.937816</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>0.894572</td>\n",
              "      <td>-0.426294</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>-0.298268</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120591</th>\n",
              "      <td>-1.267496</td>\n",
              "      <td>0.866332</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>0.937816</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>0.894572</td>\n",
              "      <td>1.145754</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>-2.214928</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>1.241872</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>0.107241</td>\n",
              "      <td>-1.465640</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>1.954418</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120592</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>-0.194625</td>\n",
              "      <td>-0.227692</td>\n",
              "      <td>0.084203</td>\n",
              "      <td>0.005122</td>\n",
              "      <td>0.130379</td>\n",
              "      <td>-0.014408</td>\n",
              "      <td>0.359730</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>-2.214928</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>1.241872</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>-1.109287</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120593</th>\n",
              "      <td>-1.267496</td>\n",
              "      <td>0.866332</td>\n",
              "      <td>0.741414</td>\n",
              "      <td>0.937816</td>\n",
              "      <td>0.973071</td>\n",
              "      <td>0.941114</td>\n",
              "      <td>-0.014408</td>\n",
              "      <td>0.359730</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>4.119886</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>1.323770</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120594</th>\n",
              "      <td>0.309904</td>\n",
              "      <td>0.866332</td>\n",
              "      <td>-1.196797</td>\n",
              "      <td>-0.769410</td>\n",
              "      <td>0.005122</td>\n",
              "      <td>-1.491089</td>\n",
              "      <td>-1.832366</td>\n",
              "      <td>-1.998341</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>0.329666</td>\n",
              "      <td>...</td>\n",
              "      <td>4.119886</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>-1.514796</td>\n",
              "      <td>-1.465640</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>1.954418</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120595</th>\n",
              "      <td>-1.267496</td>\n",
              "      <td>-3.377497</td>\n",
              "      <td>-3.135009</td>\n",
              "      <td>-2.476635</td>\n",
              "      <td>-2.898725</td>\n",
              "      <td>-2.301823</td>\n",
              "      <td>-2.741345</td>\n",
              "      <td>-1.998341</td>\n",
              "      <td>0.619856</td>\n",
              "      <td>-4.759523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242725</td>\n",
              "      <td>-0.243223</td>\n",
              "      <td>-0.805236</td>\n",
              "      <td>-0.289899</td>\n",
              "      <td>0.512751</td>\n",
              "      <td>0.682296</td>\n",
              "      <td>-0.35187</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.030219</td>\n",
              "      <td>0.030219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120566 rows × 85 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Q1        Q2        Q3        Q4        Q5        Q6        Q7  \\\n",
              "0       0.309904  0.866332  0.741414  0.937816  0.973071  0.941114  0.894572   \n",
              "1       0.309904 -0.194625  0.741414 -0.769410  0.973071  0.941114  0.894572   \n",
              "2       0.309904  0.866332 -2.165903  0.084203  0.973071 -1.491089 -0.014408   \n",
              "3       0.309904 -0.194625  0.741414  0.084203 -0.962827  0.941114 -0.014408   \n",
              "4       0.309904 -0.194625  0.741414  0.937816  0.973071  0.941114  0.894572   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "120591 -1.267496  0.866332  0.741414  0.937816  0.973071  0.941114  0.894572   \n",
              "120592  0.309904 -0.194625 -0.227692  0.084203  0.005122  0.130379 -0.014408   \n",
              "120593 -1.267496  0.866332  0.741414  0.937816  0.973071  0.941114 -0.014408   \n",
              "120594  0.309904  0.866332 -1.196797 -0.769410  0.005122 -1.491089 -1.832366   \n",
              "120595 -1.267496 -3.377497 -3.135009 -2.476635 -2.898725 -2.301823 -2.741345   \n",
              "\n",
              "              Q8        Q9       Q10  ...  \\\n",
              "0       1.145754 -0.762282  0.329666  ...   \n",
              "1       1.145754  0.619856  0.329666  ...   \n",
              "2      -0.426294  0.619856  0.329666  ...   \n",
              "3       0.359730  0.619856  0.329666  ...   \n",
              "4      -0.426294  0.619856  0.329666  ...   \n",
              "...          ...       ...       ...  ...   \n",
              "120591  1.145754  0.619856 -2.214928  ...   \n",
              "120592  0.359730  0.619856 -2.214928  ...   \n",
              "120593  0.359730  0.619856  0.329666  ...   \n",
              "120594 -1.998341  0.619856  0.329666  ...   \n",
              "120595 -1.998341  0.619856 -4.759523  ...   \n",
              "\n",
              "        RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto  \\\n",
              "0                                               -0.242725            \n",
              "1                                               -0.242725            \n",
              "2                                               -0.242725            \n",
              "3                                               -0.242725            \n",
              "4                                               -0.242725            \n",
              "...                                                   ...            \n",
              "120591                                          -0.242725            \n",
              "120592                                          -0.242725            \n",
              "120593                                           4.119886            \n",
              "120594                                           4.119886            \n",
              "120595                                          -0.242725            \n",
              "\n",
              "        RegiaoMetropolitana_Região Metropolitana de Sorocaba  \\\n",
              "0                                               -0.243223      \n",
              "1                                               -0.243223      \n",
              "2                                               -0.243223      \n",
              "3                                               -0.243223      \n",
              "4                                               -0.243223      \n",
              "...                                                   ...      \n",
              "120591                                          -0.243223      \n",
              "120592                                          -0.243223      \n",
              "120593                                          -0.243223      \n",
              "120594                                          -0.243223      \n",
              "120595                                          -0.243223      \n",
              "\n",
              "        RegiaoMetropolitana_Região Metropolitana de São Paulo  \\\n",
              "0                                               -0.805236       \n",
              "1                                                1.241872       \n",
              "2                                               -0.805236       \n",
              "3                                                1.241872       \n",
              "4                                               -0.805236       \n",
              "...                                                   ...       \n",
              "120591                                           1.241872       \n",
              "120592                                           1.241872       \n",
              "120593                                          -0.805236       \n",
              "120594                                          -0.805236       \n",
              "120595                                          -0.805236       \n",
              "\n",
              "        RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte  \\\n",
              "0                                                3.449478                             \n",
              "1                                               -0.289899                             \n",
              "2                                               -0.289899                             \n",
              "3                                               -0.289899                             \n",
              "4                                               -0.289899                             \n",
              "...                                                   ...                             \n",
              "120591                                          -0.289899                             \n",
              "120592                                          -0.289899                             \n",
              "120593                                          -0.289899                             \n",
              "120594                                          -0.289899                             \n",
              "120595                                          -0.289899                             \n",
              "\n",
              "           idade  PERIODO_MANHÃ  PERIODO_NOITE  PERIODO_TARDE  Tipo_PROVA_A  \\\n",
              "0       0.918260       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "1      -1.514796       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "2      -0.298268       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "3       0.918260       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "4      -0.298268       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "...          ...            ...            ...            ...           ...   \n",
              "120591  0.107241      -1.465640       -0.35187       1.954418     -0.030219   \n",
              "120592 -1.109287       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "120593  1.323770       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "120594 -1.514796      -1.465640       -0.35187       1.954418     -0.030219   \n",
              "120595  0.512751       0.682296       -0.35187      -0.511661     -0.030219   \n",
              "\n",
              "        Tipo_PROVA_C  \n",
              "0           0.030219  \n",
              "1           0.030219  \n",
              "2           0.030219  \n",
              "3           0.030219  \n",
              "4           0.030219  \n",
              "...              ...  \n",
              "120591      0.030219  \n",
              "120592      0.030219  \n",
              "120593      0.030219  \n",
              "120594      0.030219  \n",
              "120595      0.030219  \n",
              "\n",
              "[120566 rows x 85 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cie_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "porc_ACERT_CIE\n",
            "(array([ 12057,  12058,  12059, ..., 120563, 120564, 120565]), array([    0,     1,     2, ..., 12054, 12055, 12056]))\n",
            "Epoch: 0, Train Loss: 0.9991698072150091, Val Loss: 0.9767786733128294, R2: 0.023799005774421178\n",
            "Epoch: 100, Train Loss: 0.5053716443431893, Val Loss: 0.5127179831531535, R2: 0.4875852446553738\n",
            "Epoch: 200, Train Loss: 0.4838830148126254, Val Loss: 0.49176187537905597, R2: 0.5085289587260234\n",
            "Epoch: 300, Train Loss: 0.48008935361883165, Val Loss: 0.4879083230829914, R2: 0.5123802319832086\n",
            "Epoch: 400, Train Loss: 0.478995245043859, Val Loss: 0.48675537509707173, R2: 0.5135324981013129\n",
            "Epoch: 500, Train Loss: 0.4785081628487643, Val Loss: 0.4862491738373317, R2: 0.5140383999873158\n",
            "Epoch: 600, Train Loss: 0.4781913209097156, Val Loss: 0.4859402264719133, R2: 0.5143471646374099\n",
            "Epoch: 700, Train Loss: 0.477935954358074, Val Loss: 0.4857067204888082, R2: 0.5145805325221648\n",
            "Epoch: 800, Train Loss: 0.4777109100086618, Val Loss: 0.4855098499317113, R2: 0.5147772866475595\n",
            "Epoch: 900, Train Loss: 0.4775057521367363, Val Loss: 0.48533544143347246, R2: 0.5149515919984378\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([12057, 12058, 12059, ..., 24111, 24112, 24113]))\n",
            "Epoch: 0, Train Loss: 1.0024707535219841, Val Loss: 0.9688476765947901, R2: 0.02184154944033012\n",
            "Epoch: 100, Train Loss: 0.5067060043741112, Val Loss: 0.5015213766043243, R2: 0.4936589264619634\n",
            "Epoch: 200, Train Loss: 0.48521517974586814, Val Loss: 0.4800494257511381, R2: 0.515337226038286\n",
            "Epoch: 300, Train Loss: 0.48145930041877055, Val Loss: 0.4759630432327622, R2: 0.5194628793160205\n",
            "Epoch: 400, Train Loss: 0.48039178190796794, Val Loss: 0.4746666539902743, R2: 0.5207717270568862\n",
            "Epoch: 500, Train Loss: 0.47992364960255485, Val Loss: 0.4740364527156586, R2: 0.5214079846618851\n",
            "Epoch: 600, Train Loss: 0.47962205607079184, Val Loss: 0.4736115689384196, R2: 0.5218369516370371\n",
            "Epoch: 700, Train Loss: 0.47938005600040795, Val Loss: 0.47327090985380516, R2: 0.5221808845074206\n",
            "Epoch: 800, Train Loss: 0.47916724435994734, Val Loss: 0.4729762920064099, R2: 0.5224783336772718\n",
            "Epoch: 900, Train Loss: 0.47897350251170145, Val Loss: 0.4727127921206012, R2: 0.5227443658371786\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([24114, 24115, 24116, ..., 36168, 36169, 36170]))\n",
            "Epoch: 0, Train Loss: 1.0021176681423776, Val Loss: 0.9632869488107455, R2: 0.022895284449259257\n",
            "Epoch: 100, Train Loss: 0.5056849312285718, Val Loss: 0.5070370294544512, R2: 0.4856898320376606\n",
            "Epoch: 200, Train Loss: 0.48415572756476677, Val Loss: 0.4883289756508613, R2: 0.5046662415206629\n",
            "Epoch: 300, Train Loss: 0.4803889452159126, Val Loss: 0.4850086168306633, R2: 0.5080342288732802\n",
            "Epoch: 400, Train Loss: 0.47930925192640106, Val Loss: 0.48398529461457346, R2: 0.5090722300255885\n",
            "Epoch: 500, Train Loss: 0.47882980809610354, Val Loss: 0.48349035990072786, R2: 0.5095742642775687\n",
            "Epoch: 600, Train Loss: 0.4785177575189462, Val Loss: 0.4831562521428306, R2: 0.50991316460851\n",
            "Epoch: 700, Train Loss: 0.4782659472083433, Val Loss: 0.48288871044783765, R2: 0.5101845440267683\n",
            "Epoch: 800, Train Loss: 0.47804386174619096, Val Loss: 0.48265838449004117, R2: 0.5104181738706632\n",
            "Epoch: 900, Train Loss: 0.47784132845755567, Val Loss: 0.4824538621735518, R2: 0.5106256299356746\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([36171, 36172, 36173, ..., 48225, 48226, 48227]))\n",
            "Epoch: 0, Train Loss: 1.0008122350318915, Val Loss: 0.9687866075003024, R2: 0.023270802710516403\n",
            "Epoch: 100, Train Loss: 0.505948280849049, Val Loss: 0.5062830501007816, R2: 0.4895661920821368\n",
            "Epoch: 200, Train Loss: 0.48436591814775254, Val Loss: 0.48632225696693965, R2: 0.5096906336298827\n",
            "Epoch: 300, Train Loss: 0.48056240235077635, Val Loss: 0.4830375410198242, R2: 0.5130022792962364\n",
            "Epoch: 400, Train Loss: 0.4794734755143848, Val Loss: 0.4821628149233344, R2: 0.5138841768281162\n",
            "Epoch: 500, Train Loss: 0.47899362475170715, Val Loss: 0.48176649127126536, R2: 0.5142837497366961\n",
            "Epoch: 600, Train Loss: 0.47868374350195825, Val Loss: 0.48148141766298685, R2: 0.5145711605188299\n",
            "Epoch: 700, Train Loss: 0.4784349426096438, Val Loss: 0.48123176493615283, R2: 0.5148228600216862\n",
            "Epoch: 800, Train Loss: 0.47821623599248025, Val Loss: 0.4810017039573324, R2: 0.5150548071537288\n",
            "Epoch: 900, Train Loss: 0.4780173130421322, Val Loss: 0.4807876854000959, R2: 0.5152705803405134\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([48228, 48229, 48230, ..., 60282, 60283, 60284]))\n",
            "Epoch: 0, Train Loss: 1.0005786462276849, Val Loss: 0.9619472673333207, R2: 0.024365366981856984\n",
            "Epoch: 100, Train Loss: 0.5065668828597372, Val Loss: 0.5007073677513934, R2: 0.4921681618371496\n",
            "Epoch: 200, Train Loss: 0.485092857250421, Val Loss: 0.48014425295128194, R2: 0.513023865307603\n",
            "Epoch: 300, Train Loss: 0.48132214902616044, Val Loss: 0.47648559102614074, R2: 0.5167345856411281\n",
            "Epoch: 400, Train Loss: 0.48024019973647647, Val Loss: 0.475431452212636, R2: 0.5178037235963873\n",
            "Epoch: 500, Train Loss: 0.4797600142428449, Val Loss: 0.47496105531037625, R2: 0.518280813687179\n",
            "Epoch: 600, Train Loss: 0.47944867150723747, Val Loss: 0.47465415758719653, R2: 0.5185920782000027\n",
            "Epoch: 700, Train Loss: 0.47919872079391423, Val Loss: 0.47440645728043257, R2: 0.5188433029875714\n",
            "Epoch: 800, Train Loss: 0.4789793047863496, Val Loss: 0.4741876012175981, R2: 0.5190652731119206\n",
            "Epoch: 900, Train Loss: 0.47877998479403827, Val Loss: 0.4739870384263451, R2: 0.5192686896731877\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([60285, 60286, 60287, ..., 72339, 72340, 72341]))\n",
            "Epoch: 0, Train Loss: 0.9985297791609575, Val Loss: 0.9778697191257025, R2: 0.02487392302393432\n",
            "Epoch: 100, Train Loss: 0.5072821968201346, Val Loss: 0.4955531397707354, R2: 0.5058372504367408\n",
            "Epoch: 200, Train Loss: 0.4857929345062714, Val Loss: 0.4737254528702941, R2: 0.5276036946576739\n",
            "Epoch: 300, Train Loss: 0.48200009176194797, Val Loss: 0.47002296584174497, R2: 0.531295793493122\n",
            "Epoch: 400, Train Loss: 0.4809050355906172, Val Loss: 0.46905534970926777, R2: 0.5322606947522844\n",
            "Epoch: 500, Train Loss: 0.4804162762549178, Val Loss: 0.4686707483994765, R2: 0.5326442169731709\n",
            "Epoch: 600, Train Loss: 0.480098107637077, Val Loss: 0.46843227423327977, R2: 0.5328820220443324\n",
            "Epoch: 700, Train Loss: 0.4798420286802514, Val Loss: 0.4682382867316779, R2: 0.5330754652686391\n",
            "Epoch: 800, Train Loss: 0.47961684073758376, Val Loss: 0.468063397279289, R2: 0.5332498640277754\n",
            "Epoch: 900, Train Loss: 0.47941200701585485, Val Loss: 0.46790094421175926, R2: 0.5334118612952381\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([72342, 72343, 72344, ..., 84395, 84396, 84397]))\n",
            "Epoch: 0, Train Loss: 0.9982647863802788, Val Loss: 0.9952436272257602, R2: 0.02292862586968114\n",
            "Epoch: 100, Train Loss: 0.5048307832582377, Val Loss: 0.5168044587702284, R2: 0.4926319256171843\n",
            "Epoch: 200, Train Loss: 0.4833566215182382, Val Loss: 0.495687026240939, R2: 0.5133637728303256\n",
            "Epoch: 300, Train Loss: 0.4795734788832919, Val Loss: 0.4921120024910189, R2: 0.5168735198634414\n",
            "Epoch: 400, Train Loss: 0.4784833356831177, Val Loss: 0.49114517293216375, R2: 0.517822695984522\n",
            "Epoch: 500, Train Loss: 0.4779975408040192, Val Loss: 0.49073697756600343, R2: 0.5182234380705977\n",
            "Epoch: 600, Train Loss: 0.4776812589761695, Val Loss: 0.4904733441695805, R2: 0.5184822577584239\n",
            "Epoch: 700, Train Loss: 0.4774263957865387, Val Loss: 0.4902575141388254, R2: 0.5186941469270251\n",
            "Epoch: 800, Train Loss: 0.4772019586196727, Val Loss: 0.4900646827000959, R2: 0.5188834574371965\n",
            "Epoch: 900, Train Loss: 0.4769975208563453, Val Loss: 0.4898877482700514, R2: 0.5190571612037571\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([84398, 84399, 84400, ..., 96451, 96452, 96453]))\n",
            "Epoch: 0, Train Loss: 0.9990655568757779, Val Loss: 0.9833552821866076, R2: 0.02327624770406611\n",
            "Epoch: 100, Train Loss: 0.5053840583799725, Val Loss: 0.511943824940396, R2: 0.49150861065326745\n",
            "Epoch: 200, Train Loss: 0.4838998921394162, Val Loss: 0.49087673527622183, R2: 0.5124336285379452\n",
            "Epoch: 300, Train Loss: 0.480131661836885, Val Loss: 0.48722471893972286, R2: 0.5160610164863779\n",
            "Epoch: 400, Train Loss: 0.47905741140494085, Val Loss: 0.48620390365923755, R2: 0.5170749476150536\n",
            "Epoch: 500, Train Loss: 0.4785859903058476, Val Loss: 0.4857440628206707, R2: 0.5175316873890091\n",
            "Epoch: 600, Train Loss: 0.47828279774451454, Val Loss: 0.4854256819942768, R2: 0.5178479211257345\n",
            "Epoch: 700, Train Loss: 0.47804005173942005, Val Loss: 0.4851523345067543, R2: 0.5181194252184363\n",
            "Epoch: 800, Train Loss: 0.4778269583515524, Val Loss: 0.48490054224546647, R2: 0.5183695194485678\n",
            "Epoch: 900, Train Loss: 0.477633209463291, Val Loss: 0.484664333630629, R2: 0.5186041351249037\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([ 96454,  96455,  96456, ..., 108507, 108508, 108509]))\n",
            "Epoch: 0, Train Loss: 0.999787063354865, Val Loss: 0.9762071752020058, R2: 0.023274545320986273\n",
            "Epoch: 100, Train Loss: 0.5063247436678252, Val Loss: 0.5052853714843142, R2: 0.4944463667729688\n",
            "Epoch: 200, Train Loss: 0.48476183872325146, Val Loss: 0.4836278183793559, R2: 0.5161154181188554\n",
            "Epoch: 300, Train Loss: 0.48095131385288764, Val Loss: 0.4798556090591558, R2: 0.5198896301478282\n",
            "Epoch: 400, Train Loss: 0.4798550020246192, Val Loss: 0.4788098316116463, R2: 0.5209359628104624\n",
            "Epoch: 500, Train Loss: 0.47936860768259426, Val Loss: 0.4783684743576417, R2: 0.521377554386046\n",
            "Epoch: 600, Train Loss: 0.47905355536776356, Val Loss: 0.4780908909230983, R2: 0.5216552851927876\n",
            "Epoch: 700, Train Loss: 0.47880073983728255, Val Loss: 0.4778675182549137, R2: 0.5218787764520588\n",
            "Epoch: 800, Train Loss: 0.4785788260564826, Val Loss: 0.4776676913858604, R2: 0.5220787094114665\n",
            "Epoch: 900, Train Loss: 0.47837723259952436, Val Loss: 0.4774820610748132, R2: 0.5222644382757542\n",
            "(array([     0,      1,      2, ..., 108507, 108508, 108509]), array([108510, 108511, 108512, ..., 120563, 120564, 120565]))\n",
            "Epoch: 0, Train Loss: 0.9989136266385096, Val Loss: 0.993525364229814, R2: 0.023054003775591303\n",
            "Epoch: 100, Train Loss: 0.5067137728690957, Val Loss: 0.5021316420845469, R2: 0.5062476359701525\n",
            "Epoch: 200, Train Loss: 0.4851230282771533, Val Loss: 0.48017063288690226, R2: 0.5278421727390433\n",
            "Epoch: 300, Train Loss: 0.48130095055297456, Val Loss: 0.4764650383577342, R2: 0.5314859304821666\n",
            "Epoch: 400, Train Loss: 0.4801961493508471, Val Loss: 0.4754954000439026, R2: 0.5324393880410665\n",
            "Epoch: 500, Train Loss: 0.47970379880989783, Val Loss: 0.47511732107581145, R2: 0.5328111578492984\n",
            "Epoch: 600, Train Loss: 0.4793841533126891, Val Loss: 0.4748897435757348, R2: 0.5330349376696586\n",
            "Epoch: 700, Train Loss: 0.47912748579585823, Val Loss: 0.4747065196665478, R2: 0.5332151040458151\n",
            "Epoch: 800, Train Loss: 0.47890217773683186, Val Loss: 0.47454005507418956, R2: 0.5333787907747831\n",
            "Epoch: 900, Train Loss: 0.47869751724599435, Val Loss: 0.47438309507892096, R2: 0.5335331315137978\n",
            "porc_ACERT_MAT\n",
            "(array([ 12057,  12058,  12059, ..., 120563, 120564, 120565]), array([    0,     1,     2, ..., 12054, 12055, 12056]))\n",
            "Epoch: 0, Train Loss: 1.0000806626835046, Val Loss: 0.969964416670107, R2: 0.028901743334702856\n",
            "Epoch: 100, Train Loss: 0.4585450029166053, Val Loss: 0.461641690253489, R2: 0.5378186736497008\n",
            "Epoch: 200, Train Loss: 0.44282433424819273, Val Loss: 0.44617049068681286, R2: 0.5533079582765512\n",
            "Epoch: 300, Train Loss: 0.44066188496579073, Val Loss: 0.4439253953259553, R2: 0.5555556780418398\n",
            "Epoch: 400, Train Loss: 0.440172046146838, Val Loss: 0.44338788982791216, R2: 0.5560938118569849\n",
            "Epoch: 500, Train Loss: 0.4400186193762741, Val Loss: 0.4432089505677484, R2: 0.5562729602882136\n",
            "Epoch: 600, Train Loss: 0.43995586557893546, Val Loss: 0.44313121556399526, R2: 0.5563507861603065\n",
            "Epoch: 700, Train Loss: 0.43992404311448235, Val Loss: 0.44309041833733925, R2: 0.5563916310768846\n",
            "Epoch: 800, Train Loss: 0.43990496337578794, Val Loss: 0.4430664713411374, R2: 0.5564156060659298\n",
            "Epoch: 900, Train Loss: 0.43989192475951033, Val Loss: 0.4430515857161944, R2: 0.5564305090914254\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([12057, 12058, 12059, ..., 24111, 24112, 24113]))\n",
            "Epoch: 0, Train Loss: 1.0009774594585565, Val Loss: 0.9669419058052052, R2: 0.02841748180708159\n",
            "Epoch: 100, Train Loss: 0.4586905567690188, Val Loss: 0.4592385159865493, R2: 0.5385574757546221\n",
            "Epoch: 200, Train Loss: 0.44267709116885806, Val Loss: 0.4462822353716446, R2: 0.5515759370196354\n",
            "Epoch: 300, Train Loss: 0.44045398224195403, Val Loss: 0.44519867783384093, R2: 0.5526646948393821\n",
            "Epoch: 400, Train Loss: 0.43994968386902383, Val Loss: 0.4451304230682677, R2: 0.5527332771777999\n",
            "Epoch: 500, Train Loss: 0.43979235870013794, Val Loss: 0.44514378780406055, R2: 0.5527198483010715\n",
            "Epoch: 600, Train Loss: 0.4397289328305759, Val Loss: 0.44514911067396795, R2: 0.5527144998852901\n",
            "Epoch: 700, Train Loss: 0.439697675354372, Val Loss: 0.4451461899137542, R2: 0.5527174346630119\n",
            "Epoch: 800, Train Loss: 0.4396796745897977, Val Loss: 0.4451398567210048, R2: 0.5527237982504445\n",
            "Epoch: 900, Train Loss: 0.4396679152283979, Val Loss: 0.44513256043393234, R2: 0.5527311295543467\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([24114, 24115, 24116, ..., 36168, 36169, 36170]))\n",
            "Epoch: 0, Train Loss: 0.9993749906689606, Val Loss: 0.9767798749452129, R2: 0.028813873376460353\n",
            "Epoch: 100, Train Loss: 0.4582069163048489, Val Loss: 0.46465779481517794, R2: 0.5380031719252001\n",
            "Epoch: 200, Train Loss: 0.44239402096071584, Val Loss: 0.44988865573018616, R2: 0.5526877322334496\n",
            "Epoch: 300, Train Loss: 0.44022166633024573, Val Loss: 0.44783906939743684, R2: 0.5547255811518612\n",
            "Epoch: 400, Train Loss: 0.43973270778638834, Val Loss: 0.4473379436932497, R2: 0.5552238370476723\n",
            "Epoch: 500, Train Loss: 0.4395812958444306, Val Loss: 0.44715610119087734, R2: 0.5554046381883023\n",
            "Epoch: 600, Train Loss: 0.43952037503989394, Val Loss: 0.44706784558696316, R2: 0.5554923883767704\n",
            "Epoch: 700, Train Loss: 0.4394901513543351, Val Loss: 0.4470163428189529, R2: 0.5555435962026657\n",
            "Epoch: 800, Train Loss: 0.4394725011129851, Val Loss: 0.44698271832180964, R2: 0.555577028141609\n",
            "Epoch: 900, Train Loss: 0.4394607676479066, Val Loss: 0.44695906352122444, R2: 0.5556005474776904\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([36171, 36172, 36173, ..., 48225, 48226, 48227]))\n",
            "Epoch: 0, Train Loss: 1.0021327697777331, Val Loss: 0.9604738183870383, R2: 0.027904617474272975\n",
            "Epoch: 100, Train Loss: 0.4589690448278077, Val Loss: 0.458300536666739, R2: 0.5361541075102232\n",
            "Epoch: 200, Train Loss: 0.44314077223811893, Val Loss: 0.443381511955111, R2: 0.5512536497947966\n",
            "Epoch: 300, Train Loss: 0.4409584503517894, Val Loss: 0.4413345993088389, R2: 0.5533253297237885\n",
            "Epoch: 400, Train Loss: 0.4404676422848706, Val Loss: 0.4408581841480455, R2: 0.5538075094241682\n",
            "Epoch: 500, Train Loss: 0.4403168291855502, Val Loss: 0.4406842642454451, R2: 0.5539835337269765\n",
            "Epoch: 600, Train Loss: 0.44025707391407265, Val Loss: 0.44059099070964775, R2: 0.554077935856139\n",
            "Epoch: 700, Train Loss: 0.44022804663734544, Val Loss: 0.44052827446623277, R2: 0.5541414109550598\n",
            "Epoch: 800, Train Loss: 0.4402115053679665, Val Loss: 0.4404815766096035, R2: 0.5541886738475345\n",
            "Epoch: 900, Train Loss: 0.44020078636126536, Val Loss: 0.44044520347626653, R2: 0.5542254870893824\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([48228, 48229, 48230, ..., 60282, 60283, 60284]))\n",
            "Epoch: 0, Train Loss: 0.9977847486952767, Val Loss: 0.9832873471220043, R2: 0.029508300039232216\n",
            "Epoch: 100, Train Loss: 0.4582738945759975, Val Loss: 0.4650189784762434, R2: 0.5410323744566266\n",
            "Epoch: 200, Train Loss: 0.4425065324897818, Val Loss: 0.44890634913899413, R2: 0.5569353280358762\n",
            "Epoch: 300, Train Loss: 0.4403227342912955, Val Loss: 0.4467307018748526, R2: 0.5590826633169328\n",
            "Epoch: 400, Train Loss: 0.43982266642091933, Val Loss: 0.44629049448553343, R2: 0.5595171422297818\n",
            "Epoch: 500, Train Loss: 0.4396637618345701, Val Loss: 0.44619733553924146, R2: 0.5596090888864921\n",
            "Epoch: 600, Train Loss: 0.43959814648131945, Val Loss: 0.4461908588115014, R2: 0.5596154813317249\n",
            "Epoch: 700, Train Loss: 0.4395649403191118, Val Loss: 0.446206940105321, R2: 0.5595996093058901\n",
            "Epoch: 800, Train Loss: 0.439545293942579, Val Loss: 0.446226995850641, R2: 0.5595798145486134\n",
            "Epoch: 900, Train Loss: 0.43953213069127783, Val Loss: 0.44624535120464753, R2: 0.559561698055235\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([60285, 60286, 60287, ..., 72339, 72340, 72341]))\n",
            "Epoch: 0, Train Loss: 0.9978244268942356, Val Loss: 0.974891473791977, R2: 0.03081285716931681\n",
            "Epoch: 100, Train Loss: 0.4600996966990194, Val Loss: 0.44975374036892213, R2: 0.5528778799243246\n",
            "Epoch: 200, Train Loss: 0.4442873570655058, Val Loss: 0.4332506756876897, R2: 0.5692843810953119\n",
            "Epoch: 300, Train Loss: 0.44210631066529044, Val Loss: 0.4309274165617768, R2: 0.5715940462578768\n",
            "Epoch: 400, Train Loss: 0.44161117919088666, Val Loss: 0.4303863830461483, R2: 0.5721319140526868\n",
            "Epoch: 500, Train Loss: 0.44145606381264046, Val Loss: 0.43021581593684904, R2: 0.5723014831316733\n",
            "Epoch: 600, Train Loss: 0.44139318029924757, Val Loss: 0.4301510242403001, R2: 0.5723658957159922\n",
            "Epoch: 700, Train Loss: 0.4413619751492672, Val Loss: 0.4301234680116326, R2: 0.5723932907063551\n",
            "Epoch: 800, Train Loss: 0.44134385769207246, Val Loss: 0.4301105117335345, R2: 0.5724061711740679\n",
            "Epoch: 900, Train Loss: 0.4413319226711392, Val Loss: 0.43010342828681786, R2: 0.5724132131737871\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([72342, 72343, 72344, ..., 84395, 84396, 84397]))\n",
            "Epoch: 0, Train Loss: 1.0011152119213784, Val Loss: 0.952734428472954, R2: 0.030331150865933743\n",
            "Epoch: 100, Train Loss: 0.4599705321458344, Val Loss: 0.447645066243239, R2: 0.5443982465288367\n",
            "Epoch: 200, Train Loss: 0.4441350494091447, Val Loss: 0.43351172430636403, R2: 0.5587828021833581\n",
            "Epoch: 300, Train Loss: 0.4419414883348636, Val Loss: 0.43187220900834844, R2: 0.5604514591192815\n",
            "Epoch: 400, Train Loss: 0.44144274190447036, Val Loss: 0.43164130495175385, R2: 0.5606864673903454\n",
            "Epoch: 500, Train Loss: 0.44128688525337617, Val Loss: 0.4316198235815521, R2: 0.5607083305821214\n",
            "Epoch: 600, Train Loss: 0.4412237867011303, Val Loss: 0.4316239206922164, R2: 0.5607041606471806\n",
            "Epoch: 700, Train Loss: 0.441192356978831, Val Loss: 0.4316259656955118, R2: 0.5607020792948828\n",
            "Epoch: 800, Train Loss: 0.4411739413731253, Val Loss: 0.43162426202258364, R2: 0.5607038132498215\n",
            "Epoch: 900, Train Loss: 0.44116165903097804, Val Loss: 0.43162045373223085, R2: 0.5607076892308014\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([84398, 84399, 84400, ..., 96451, 96452, 96453]))\n",
            "Epoch: 0, Train Loss: 1.0002649496411145, Val Loss: 0.9681315898178579, R2: 0.028718434338370957\n",
            "Epoch: 100, Train Loss: 0.4582222052200529, Val Loss: 0.4645011379999424, R2: 0.5339875309170802\n",
            "Epoch: 200, Train Loss: 0.44241856755662046, Val Loss: 0.4496921508099099, R2: 0.5488447015900297\n",
            "Epoch: 300, Train Loss: 0.440252445328588, Val Loss: 0.4475677568211459, R2: 0.5509760076451053\n",
            "Epoch: 400, Train Loss: 0.43976523919236227, Val Loss: 0.44704071415099866, R2: 0.551504765135528\n",
            "Epoch: 500, Train Loss: 0.4396136576584819, Val Loss: 0.44686133834879543, R2: 0.5516847245664961\n",
            "Epoch: 600, Train Loss: 0.4395522744845707, Val Loss: 0.4467866391349672, R2: 0.5517596668265399\n",
            "Epoch: 700, Train Loss: 0.4395216845559445, Val Loss: 0.4467513239202383, R2: 0.5517950969451461\n",
            "Epoch: 800, Train Loss: 0.43950379137540596, Val Loss: 0.4467329584755538, R2: 0.5518135221448346\n",
            "Epoch: 900, Train Loss: 0.43949190263314947, Val Loss: 0.44672239069600145, R2: 0.5518241243084011\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([ 96454,  96455,  96456, ..., 108507, 108508, 108509]))\n",
            "Epoch: 0, Train Loss: 1.0007984035361257, Val Loss: 0.9713026087532158, R2: 0.02802285730448162\n",
            "Epoch: 100, Train Loss: 0.45901260485080336, Val Loss: 0.45876358276943635, R2: 0.5409178227932775\n",
            "Epoch: 200, Train Loss: 0.44318900747076995, Val Loss: 0.4427919182881174, R2: 0.5569005790082237\n",
            "Epoch: 300, Train Loss: 0.44099447602521846, Val Loss: 0.440634491094202, R2: 0.5590595044559863\n",
            "Epoch: 400, Train Loss: 0.44049510427291816, Val Loss: 0.4402182426994021, R2: 0.5594760419199889\n",
            "Epoch: 500, Train Loss: 0.4403388261454974, Val Loss: 0.44013138394263335, R2: 0.55956296099697\n",
            "Epoch: 600, Train Loss: 0.4402755881925313, Val Loss: 0.4401165015265665, R2: 0.5595778537483276\n",
            "Epoch: 700, Train Loss: 0.44024424058527145, Val Loss: 0.4401170251995925, R2: 0.5595773297116298\n",
            "Epoch: 800, Train Loss: 0.44022604190218284, Val Loss: 0.440119524772341, R2: 0.55957482840302\n",
            "Epoch: 900, Train Loss: 0.4402140461163239, Val Loss: 0.4401209892561393, R2: 0.5595733629021915\n",
            "(array([     0,      1,      2, ..., 108507, 108508, 108509]), array([108510, 108511, 108512, ..., 120563, 120564, 120565]))\n",
            "Epoch: 0, Train Loss: 0.9982740286161402, Val Loss: 0.9838814251959698, R2: 0.029513996177023993\n",
            "Epoch: 100, Train Loss: 0.45920791510834885, Val Loss: 0.45910023877102707, R2: 0.5471503530109603\n",
            "Epoch: 200, Train Loss: 0.4434718683867671, Val Loss: 0.44162972307399545, R2: 0.5643830098427138\n",
            "Epoch: 300, Train Loss: 0.4412949035123385, Val Loss: 0.43864901892543345, R2: 0.5673231320806557\n",
            "Epoch: 400, Train Loss: 0.44079954581579894, Val Loss: 0.43784508003947414, R2: 0.568116125440193\n",
            "Epoch: 500, Train Loss: 0.44064462391694625, Val Loss: 0.43757587896898853, R2: 0.568381661372095\n",
            "Epoch: 600, Train Loss: 0.44058201022647353, Val Loss: 0.43747242659683006, R2: 0.5684837052532691\n",
            "Epoch: 700, Train Loss: 0.4405510203918685, Val Loss: 0.4374278696444696, R2: 0.5685276555683492\n",
            "Epoch: 800, Train Loss: 0.44053306115991747, Val Loss: 0.4374061108876424, R2: 0.5685491180824496\n",
            "Epoch: 900, Train Loss: 0.44052124433941514, Val Loss: 0.43739370037853376, R2: 0.5685613596240437\n",
            "porc_ACERT_lp\n",
            "(array([ 12057,  12058,  12059, ..., 120563, 120564, 120565]), array([    0,     1,     2, ..., 12054, 12055, 12056]))\n",
            "Epoch: 0, Train Loss: 0.9993999251955705, Val Loss: 0.976498818755145, R2: 0.02741580006952382\n",
            "Epoch: 100, Train Loss: 0.46562751323909357, Val Loss: 0.4709526150540455, R2: 0.530935354431435\n",
            "Epoch: 200, Train Loss: 0.444403964376816, Val Loss: 0.44906957213860516, R2: 0.5527306719240767\n",
            "Epoch: 300, Train Loss: 0.44065079634815374, Val Loss: 0.4448581086588601, R2: 0.5569252523580868\n",
            "Epoch: 400, Train Loss: 0.43972415501014184, Val Loss: 0.4437021870359979, R2: 0.5580765400864087\n",
            "Epoch: 500, Train Loss: 0.4394491840620491, Val Loss: 0.4433059430007738, R2: 0.5584711956462249\n",
            "Epoch: 600, Train Loss: 0.43934655320747124, Val Loss: 0.4431344027910466, R2: 0.5586420481802261\n",
            "Epoch: 700, Train Loss: 0.4392951520438454, Val Loss: 0.44304140619427324, R2: 0.5587346719693509\n",
            "Epoch: 800, Train Loss: 0.4392612374644523, Val Loss: 0.4429809395118621, R2: 0.5587948962510456\n",
            "Epoch: 900, Train Loss: 0.4392344603625278, Val Loss: 0.4429362089987083, R2: 0.5588394474471277\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([12057, 12058, 12059, ..., 24111, 24112, 24113]))\n",
            "Epoch: 0, Train Loss: 1.0002062874528488, Val Loss: 0.9784249484553416, R2: 0.02597321859035784\n",
            "Epoch: 100, Train Loss: 0.46571727578632877, Val Loss: 0.4700214858501034, R2: 0.5320913312986214\n",
            "Epoch: 200, Train Loss: 0.4445810314729083, Val Loss: 0.4475897268088612, R2: 0.5544222562150305\n",
            "Epoch: 300, Train Loss: 0.44085446966604697, Val Loss: 0.4431195064258006, R2: 0.5588723822863028\n",
            "Epoch: 400, Train Loss: 0.4399319447170756, Val Loss: 0.44184529988639965, R2: 0.5601408610759976\n",
            "Epoch: 500, Train Loss: 0.4396559484752813, Val Loss: 0.4414161672572498, R2: 0.5605680646895488\n",
            "Epoch: 600, Train Loss: 0.43955160848996144, Val Loss: 0.4412502194812222, R2: 0.5607332665054074\n",
            "Epoch: 700, Train Loss: 0.4394986787033677, Val Loss: 0.44117574120060316, R2: 0.5608074099949727\n",
            "Epoch: 800, Train Loss: 0.4394634832637964, Val Loss: 0.44113558587098545, R2: 0.5608473848203548\n",
            "Epoch: 900, Train Loss: 0.43943561456590174, Val Loss: 0.4411089878373694, R2: 0.5608738632918163\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([24114, 24115, 24116, ..., 36168, 36169, 36170]))\n",
            "Epoch: 0, Train Loss: 1.0001226567493908, Val Loss: 0.9777525828354199, R2: 0.027120837659681096\n",
            "Epoch: 100, Train Loss: 0.46642586421431126, Val Loss: 0.4618108728223202, R2: 0.5404909350296685\n",
            "Epoch: 200, Train Loss: 0.4451074801186213, Val Loss: 0.4413648259309955, R2: 0.560835072515983\n",
            "Epoch: 300, Train Loss: 0.4413258325860627, Val Loss: 0.4380298391839375, R2: 0.5641534366602994\n",
            "Epoch: 400, Train Loss: 0.44038756184018296, Val Loss: 0.43731112161416774, R2: 0.5648685719199876\n",
            "Epoch: 500, Train Loss: 0.4401077818855067, Val Loss: 0.43714377103396734, R2: 0.5650350883732711\n",
            "Epoch: 600, Train Loss: 0.4400031459867609, Val Loss: 0.4370992514774777, R2: 0.5650793860305576\n",
            "Epoch: 700, Train Loss: 0.43995089726978603, Val Loss: 0.4370796673787036, R2: 0.565098872516142\n",
            "Epoch: 800, Train Loss: 0.4399166226962699, Val Loss: 0.4370627014926024, R2: 0.5651157538390332\n",
            "Epoch: 900, Train Loss: 0.43988969826408575, Val Loss: 0.4370441026498877, R2: 0.5651342599793783\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([36171, 36172, 36173, ..., 48225, 48226, 48227]))\n",
            "Epoch: 0, Train Loss: 1.0019841666297449, Val Loss: 0.9676579215826706, R2: 0.0263518381258232\n",
            "Epoch: 100, Train Loss: 0.466483549132641, Val Loss: 0.46243156391711937, R2: 0.5347057755037062\n",
            "Epoch: 200, Train Loss: 0.4452780317101236, Val Loss: 0.44080318308330935, R2: 0.5564680457992992\n",
            "Epoch: 300, Train Loss: 0.44153773717561345, Val Loss: 0.43670509932892315, R2: 0.5605914985460494\n",
            "Epoch: 400, Train Loss: 0.44061458134162623, Val Loss: 0.435606853001434, R2: 0.5616965435151392\n",
            "Epoch: 500, Train Loss: 0.440339091887401, Val Loss: 0.4352517493163465, R2: 0.562053845452682\n",
            "Epoch: 600, Train Loss: 0.4402344326164603, Val Loss: 0.4351132890067632, R2: 0.5621931628942194\n",
            "Epoch: 700, Train Loss: 0.4401805437790694, Val Loss: 0.4350471608066273, R2: 0.5622597004578298\n",
            "Epoch: 800, Train Loss: 0.44014409477823857, Val Loss: 0.4350086018556682, R2: 0.562298498106139\n",
            "Epoch: 900, Train Loss: 0.4401148908922652, Val Loss: 0.43498207369473424, R2: 0.5623251904883901\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([48228, 48229, 48230, ..., 60282, 60283, 60284]))\n",
            "Epoch: 0, Train Loss: 1.001134000078649, Val Loss: 0.9626568842572714, R2: 0.027694478844340398\n",
            "Epoch: 100, Train Loss: 0.46554770712957466, Val Loss: 0.4675028997927549, R2: 0.5278113541197135\n",
            "Epoch: 200, Train Loss: 0.4441502725494529, Val Loss: 0.44913084953291077, R2: 0.546367546045124\n",
            "Epoch: 300, Train Loss: 0.44036216283527096, Val Loss: 0.44634732635908964, R2: 0.5491789682604851\n",
            "Epoch: 400, Train Loss: 0.4394246594934872, Val Loss: 0.4458479277846874, R2: 0.549683372268901\n",
            "Epoch: 500, Train Loss: 0.4391453648285854, Val Loss: 0.44578253698887293, R2: 0.5497494184716996\n",
            "Epoch: 600, Train Loss: 0.43904090834407156, Val Loss: 0.4457883548219906, R2: 0.549743542326882\n",
            "Epoch: 700, Train Loss: 0.43898877196603575, Val Loss: 0.44579411330027435, R2: 0.5497377261318075\n",
            "Epoch: 800, Train Loss: 0.43895463008397234, Val Loss: 0.44578991149385927, R2: 0.5497419700526094\n",
            "Epoch: 900, Train Loss: 0.438927877539525, Val Loss: 0.44577764226703154, R2: 0.5497543622529666\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([60285, 60286, 60287, ..., 72339, 72340, 72341]))\n",
            "Epoch: 0, Train Loss: 1.000418564320994, Val Loss: 0.9680679950954347, R2: 0.02782176219436794\n",
            "Epoch: 100, Train Loss: 0.4666461308158284, Val Loss: 0.4605131856947721, R2: 0.5375315580897126\n",
            "Epoch: 200, Train Loss: 0.4453703036858818, Val Loss: 0.43935460904358487, R2: 0.5587799702543961\n",
            "Epoch: 300, Train Loss: 0.4416196782263044, Val Loss: 0.43559579798405834, R2: 0.5625547405500866\n",
            "Epoch: 400, Train Loss: 0.440693538111731, Val Loss: 0.4346782629793728, R2: 0.5634761712434877\n",
            "Epoch: 500, Train Loss: 0.4404167709850332, Val Loss: 0.4344109425549311, R2: 0.5637446266624043\n",
            "Epoch: 600, Train Loss: 0.4403116603841703, Val Loss: 0.4343140227423455, R2: 0.5638419579790942\n",
            "Epoch: 700, Train Loss: 0.4402577507013382, Val Loss: 0.4342677283994297, R2: 0.5638884488794692\n",
            "Epoch: 800, Train Loss: 0.44022149791953236, Val Loss: 0.4342390519148804, R2: 0.5639172471192178\n",
            "Epoch: 900, Train Loss: 0.44019259458970866, Val Loss: 0.4342178274490692, R2: 0.5639385617002959\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([72342, 72343, 72344, ..., 84395, 84396, 84397]))\n",
            "Epoch: 0, Train Loss: 1.0009173165411562, Val Loss: 0.9691250593739003, R2: 0.027254761466220523\n",
            "Epoch: 100, Train Loss: 0.4651687674024309, Val Loss: 0.4716749704988945, R2: 0.5265630815647474\n",
            "Epoch: 200, Train Loss: 0.44397256551354003, Val Loss: 0.45204916881595075, R2: 0.5462621956830278\n",
            "Epoch: 300, Train Loss: 0.4402476277182545, Val Loss: 0.4483015880350052, R2: 0.5500237756003201\n",
            "Epoch: 400, Train Loss: 0.43932995465110597, Val Loss: 0.44722271438182, R2: 0.5511066793999531\n",
            "Epoch: 500, Train Loss: 0.4390564895349275, Val Loss: 0.44682207863280193, R2: 0.5515088117289793\n",
            "Epoch: 600, Train Loss: 0.4389529213924042, Val Loss: 0.4466366391077956, R2: 0.5516949439657232\n",
            "Epoch: 700, Train Loss: 0.43889984360084594, Val Loss: 0.44653456050622914, R2: 0.5517974038832225\n",
            "Epoch: 800, Train Loss: 0.43886407031291047, Val Loss: 0.4464707835413988, R2: 0.5518614190877276\n",
            "Epoch: 900, Train Loss: 0.43883544090513293, Val Loss: 0.4464270405593323, R2: 0.5519053254722679\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([84398, 84399, 84400, ..., 96451, 96452, 96453]))\n",
            "Epoch: 0, Train Loss: 0.9999151809072202, Val Loss: 0.9661517420444462, R2: 0.028410777104322715\n",
            "Epoch: 100, Train Loss: 0.466178948593532, Val Loss: 0.4639626667549747, R2: 0.5334261615145599\n",
            "Epoch: 200, Train Loss: 0.44477175781639233, Val Loss: 0.44421201519076764, R2: 0.5532879693133482\n",
            "Epoch: 300, Train Loss: 0.4409829821518778, Val Loss: 0.440959731758444, R2: 0.5565585565256232\n",
            "Epoch: 400, Train Loss: 0.44004839190750206, Val Loss: 0.44028430155343595, R2: 0.5572377880370372\n",
            "Epoch: 500, Train Loss: 0.4397718041117058, Val Loss: 0.4401420151265244, R2: 0.5573808752488442\n",
            "Epoch: 600, Train Loss: 0.439669283545367, Val Loss: 0.44010797181833283, R2: 0.5574151101520288\n",
            "Epoch: 700, Train Loss: 0.4396185284382709, Val Loss: 0.44008972814006564, R2: 0.557433456505318\n",
            "Epoch: 800, Train Loss: 0.43958543602290934, Val Loss: 0.44006939270196493, R2: 0.557453906390819\n",
            "Epoch: 900, Train Loss: 0.43955953327889796, Val Loss: 0.44004542102292815, R2: 0.5574780129819406\n",
            "(array([     0,      1,      2, ..., 120563, 120564, 120565]), array([ 96454,  96455,  96456, ..., 108507, 108508, 108509]))\n",
            "Epoch: 0, Train Loss: 1.00156321427662, Val Loss: 0.9676564999338664, R2: 0.02653500261195063\n",
            "Epoch: 100, Train Loss: 0.46613158352097994, Val Loss: 0.4646558765416501, R2: 0.5325549596629868\n",
            "Epoch: 200, Train Loss: 0.4447474166446503, Val Loss: 0.44448743672995344, R2: 0.5528444634382997\n",
            "Epoch: 300, Train Loss: 0.4409560114705064, Val Loss: 0.4412099802134259, R2: 0.5561415933594225\n",
            "Epoch: 400, Train Loss: 0.44001665662191575, Val Loss: 0.44052714240697727, R2: 0.5568285299981124\n",
            "Epoch: 500, Train Loss: 0.43973560818528235, Val Loss: 0.4403775328420075, R2: 0.5569790376160352\n",
            "Epoch: 600, Train Loss: 0.43962919721761423, Val Loss: 0.4403401411229604, R2: 0.5570166537842591\n",
            "Epoch: 700, Train Loss: 0.439575019935922, Val Loss: 0.4403236654861891, R2: 0.5570332283184122\n",
            "Epoch: 800, Train Loss: 0.43953885295141315, Val Loss: 0.4403093249159838, R2: 0.5570476549699548\n",
            "Epoch: 900, Train Loss: 0.43951014384445364, Val Loss: 0.4402942550305207, R2: 0.5570628153145749\n",
            "(array([     0,      1,      2, ..., 108507, 108508, 108509]), array([108510, 108511, 108512, ..., 120563, 120564, 120565]))\n",
            "Epoch: 0, Train Loss: 0.9971271759904785, Val Loss: 0.9920602137133985, R2: 0.02830886214021655\n",
            "Epoch: 100, Train Loss: 0.4662506567991718, Val Loss: 0.4655582204352976, R2: 0.544000665784748\n",
            "Epoch: 200, Train Loss: 0.4448714045174915, Val Loss: 0.4437638430173523, R2: 0.5653475589465243\n",
            "Epoch: 300, Train Loss: 0.4410791491437835, Val Loss: 0.4402460833293875, R2: 0.5687930916537792\n",
            "Epoch: 400, Train Loss: 0.44014143242461445, Val Loss: 0.439498547969365, R2: 0.5695252785457944\n",
            "Epoch: 500, Train Loss: 0.439861324677873, Val Loss: 0.43931615937096924, R2: 0.569703922323922\n",
            "Epoch: 600, Train Loss: 0.43975497842810596, Val Loss: 0.4392578658058226, R2: 0.5697610189999747\n",
            "Epoch: 700, Train Loss: 0.439700369547116, Val Loss: 0.4392279870458156, R2: 0.5697902842864035\n",
            "Epoch: 800, Train Loss: 0.43966355433457244, Val Loss: 0.4392056339364674, R2: 0.5698121784395578\n",
            "Epoch: 900, Train Loss: 0.4396341279087646, Val Loss: 0.43918640712808477, R2: 0.569831010481259\n",
            "media cie: 0.5211545033844794\n",
            "media mat: 0.5591701851997585\n",
            "media lp: 0.5597364211760745\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_lin_reg(df, target):\n",
        "    \n",
        "    kf = KFold(n_splits=10)\n",
        "    y = df[target]\n",
        "    X = df.drop(columns=[target])\n",
        "    final_r2 = []\n",
        "    print(f\"{target}\")\n",
        "    \n",
        "    for i in kf.split(X):\n",
        "        print(i)\n",
        "        X_train = X.iloc[i[0]]\n",
        "        X_val = X.iloc[i[1]]\n",
        "        y_train = y.iloc[i[0]]\n",
        "        y_val = y.iloc[i[1]]\n",
        "        \n",
        "        w, b, train_loss, val_loss, r2 = linear_regression(\n",
        "        X_train, y_train, X_val, y_val, 0.01, 1000)\n",
        "        final_r2.append(r2[-1])\n",
        "    \n",
        "    return final_r2\n",
        "\n",
        "final_r2_cie = k_fold_lin_reg(df_cie_lr, 'porc_ACERT_CIE')\n",
        "final_r2_mat = k_fold_lin_reg(df_mat_lr, 'porc_ACERT_MAT')\n",
        "final_r2_lp = k_fold_lin_reg(df_lp_lr, 'porc_ACERT_lp')\n",
        "\n",
        "media_cie = 0\n",
        "for i in final_r2_cie:\n",
        "    media_cie = media_cie + i\n",
        "print(f\"media cie: {media_cie/10}\")\n",
        "\n",
        "media_mat = 0\n",
        "for i in final_r2_mat:\n",
        "    media_mat = media_mat + i\n",
        "print(f\"media mat: {media_mat/10}\")\n",
        "\n",
        "media_lp = 0\n",
        "for i in final_r2_lp:\n",
        "    media_lp = media_lp + i\n",
        "print(f\"media lp: {media_lp/10}\")\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "e4nZrMr_C2X7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5278231743443579\n",
            "0.5641338909335655\n",
            "0.5590048019083095\n",
            "CPU times: user 11 s, sys: 9 s, total: 20 s\n",
            "Wall time: 1.91 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# TODO: Linear Regression. You can use scikit-learn libraries.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "def linear_regression_sklearn(X_train, y_train):\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "model_cie = linear_regression_sklearn(X_train_cie, y_train_cie )\n",
        "model_mat = linear_regression_sklearn(X_train_mat, y_train_mat)\n",
        "model_lp = linear_regression_sklearn(X_train_lp, y_train_lp)\n",
        "print(model_cie.score(X_val_cie, y_val_cie))\n",
        "print(model_mat.score(X_val_mat, y_val_mat))\n",
        "print(model_lp.score(X_val_lp, y_val_lp))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBNZQNImKQeo"
      },
      "source": [
        "\n",
        "> What are the conclusions? (1-2 paragraphs)\n",
        "The most obvious conclusion is about time, when the scikit learn library was used to train and validate the models, the total time was 1 s, and when we used our function it took 30 seconds to train. It happened because the library is better optimized than our code, using multi thread programming and many other things.\n",
        "In terms of results, both produced the same R^2 score for all the models, showing that our solution may be as good as the one from library but much slower\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADxPBRhuK_Vq"
      },
      "source": [
        "2. (1 point) Use different Gradient Descent (GD) learning rates when optimizing. Compare the GD-based solutions with Normal Equation. What are the conclusions?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RSZ1pLItNVbU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.01\n",
            "0.5187501784822446\n",
            "0.5579244827782425\n",
            "0.5527596718112967\n",
            "Learning rate:  0.001\n",
            "0.5233539800193253\n",
            "0.5587029676946329\n",
            "0.5539154667853021\n",
            "Learning rate:  0.0001\n",
            "0.5225551205617351\n",
            "0.5593011854217729\n",
            "0.5535474364406399\n"
          ]
        }
      ],
      "source": [
        "# TODO: Gradient Descent (GD) with 3 different learning rates. You can use scikit-learn libraries.\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "def linear_regression_sklearn(X_train, y_train, X_val, y_val, learning_rate, epochs):\n",
        "    model = SGDRegressor(alpha=learning_rate, max_iter=epochs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "lrs = [0.01, 0.001, 0.0001]\n",
        "for lr in lrs:\n",
        "    print(\"Learning rate: \", lr)\n",
        "    model_cie = linear_regression_sklearn(X_train_cie, y_train_cie, X_val_cie, y_val_cie, lr, 1000)\n",
        "    model_mat = linear_regression_sklearn(X_train_mat, y_train_mat, X_val_mat, y_val_mat, lr, 1000)\n",
        "    model_lp = linear_regression_sklearn(X_train_lp, y_train_lp, X_val_lp, y_val_lp, lr, 1000)\n",
        "    print(model_cie.score(X_val_cie, y_val_cie))\n",
        "    print(model_mat.score(X_val_mat, y_val_mat))\n",
        "    print(model_lp.score(X_val_lp, y_val_lp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrPl7jKgJPW6"
      },
      "source": [
        "\n",
        "3. (0.75 point) Sometimes, we need some more complex function to make good prediction. Devise and evaluate a Polynomial Linear Regression model. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GjGbg41PMHR9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.9240610146477337e+21\n",
            "-8.074752193926429e+19\n",
            "-5.673740135754602e+18\n"
          ]
        }
      ],
      "source": [
        "# TODO: Complex model. You can use scikit-learn libraries.\n",
        "\"\"\"Multi polynomial regression\"\"\"\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "def polynomial_regression(X_train, y_train, X_val, y_val, degree):\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "model_cie = polynomial_regression(X_train_cie, y_train_cie, X_val_cie, y_val_cie, 2)\n",
        "model_mat = polynomial_regression(X_train_mat, y_train_mat, X_val_mat, y_val_mat, 2)\n",
        "model_lp = polynomial_regression(X_train_lp, y_train_lp, X_val_lp, y_val_lp, 2)\n",
        "print(model_cie.score(X_val_cie, y_val_cie))\n",
        "print(model_mat.score(X_val_mat, y_val_mat))\n",
        "print(model_lp.score(X_val_lp, y_val_lp))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBLKtosaLaCw"
      },
      "source": [
        "*texto em itálico*\n",
        " > What are the conclusions? What are the actions after such analyses? (1-2 paragraphs)\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldSh1vtWK5Zk"
      },
      "source": [
        "4. (0.5) Plot the cost function vs. number of epochs in the training/validation set and analyze the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mg7aNkl_LG4P"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Plot the cost function vs. number of iterations in the training set.\n",
        "\n",
        "def plot_cost_function(train_loss,   title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_loss, label='train loss')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_cost_function(train_loss_cie,  'Science')\n",
        "plot_cost_function(train_loss_mat,  'Math')\n",
        "plot_cost_function(train_loss_lp,  'Portuguese')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CfR862UoK9j6"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (449369750.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Input \u001b[0;32mIn [46]\u001b[0;36m\u001b[0m\n\u001b[0;31m    *texto em itálico*\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "*texto em itálico*\n",
        "> What are the conclusions? What are the actions after such analyses? (1-2 paragraphs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xij-E5UUseS"
      },
      "source": [
        "5. (0.25 point) Pick **your best model**, based on your validation set, and predict the target values for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_PobUahUseS"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Remove the columns used as target in the linear regression\"\"\"\n",
        "def tranform_into_log_reg(df):\n",
        "    df = df.drop(columns=['porc_ACERT_lp',\n",
        "       'porc_ACERT_MAT', 'porc_ACERT_CIE'], axis=1)\n",
        "    return df\n",
        "df = tranform_into_log_reg(df)\n",
        "type(df) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
              "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
              "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
              "       'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40', 'Q41',\n",
              "       'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50', 'Q51',\n",
              "       'Q52', 'Q53', 'Q54', 'Q55', 'Q56', 'Q57', 'Q58', 'Q59', 'Q60', 'Q61',\n",
              "       'Q62', 'SERIE_ANO', 'TP_SEXO', 'Tem_Nec', 'nivel_profic_lp',\n",
              "       'nivel_profic_mat', 'nivel_profic_cie', 'Q63_A', 'Q63_B', 'Q63_C',\n",
              "       'Q63_D', 'RegiaoMetropolitana_Interior',\n",
              "       'RegiaoMetropolitana_Região Metropolitana da Baixada Santista',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Campinas',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Ribeirão Preto',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de Sorocaba',\n",
              "       'RegiaoMetropolitana_Região Metropolitana de São Paulo',\n",
              "       'RegiaoMetropolitana_Região Metropolitana do Vale do Paraíba e Litoral Norte',\n",
              "       'idade', 'PERIODO_MANHÃ', 'PERIODO_NOITE', 'PERIODO_TARDE',\n",
              "       'Tipo_PROVA_A', 'Tipo_PROVA_C'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_log = df\n",
        "type(df_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['CD_ALUNO'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb Cell 76\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_log\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mCD_ALUNO\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y162sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   4955\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   4956\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   4957\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   4958\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   4959\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   4960\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   4961\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   4962\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4269\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['CD_ALUNO'] not found in axis\""
          ]
        }
      ],
      "source": [
        "df_log.drop(columns=['CD_ALUNO'], axis=1, inplace=True)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_data(df, target, validation_size):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return train_test_split(X, y, test_size=validation_size, random_state=42)\n",
        "X_train_cie, X_val_cie, y_train_cie, y_val_cie = split_data(df_log, 'nivel_profic_cie', 0.2)\n",
        "X_train_mat, X_val_mat, y_train_mat, y_val_mat = split_data(df_log, 'nivel_profic_mat', 0.2)\n",
        "X_train_lp, X_val_lp, y_train_lp, y_val_lp = split_data(df_log, 'nivel_profic_lp', 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_data(df):\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df)\n",
        "    return scaler.transform(df)\n",
        "X_train_cie = normalize_data(X_train_cie)\n",
        "X_val_cie = normalize_data(X_val_cie)\n",
        "X_train_mat = normalize_data(X_train_mat)\n",
        "X_val_mat = normalize_data(X_val_mat)\n",
        "X_train_lp = normalize_data(X_train_lp)\n",
        "X_val_lp = normalize_data(X_val_lp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X_train_cie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCJuwjrAUseS"
      },
      "source": [
        "Now, this part of the assignment aims to predict students' proeficiency level on Portuguese, Mathematics, and Natural Sciences (target values: `nivel_profic_lp`, `nivel_profic_mat` and `nivel_profic_cie`) based on their socioeconomic data. Then, you have to **drop the columns `porc_ACERT_lp`,  `porc_ACERT_MAT`** and  **`porc_ACERT_CIE`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joYtn8avUseS"
      },
      "source": [
        "### Activities\n",
        "\n",
        "1. (2.75 points) Perform Multinomial Logistic Regression (_i.e._, softmax regression). It is a generalization of Logistic Regression to the case where we want to handle multiple classes. Try different combinations of features, dropping the ones less correlated to the target variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "-36Dt2V_UseT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Science\n",
            "LogisticRegression(max_iter=1000, multi_class='multinomial', solver='sag')\n",
            "Accuracy:  0.6312100854275524\n",
            "Math\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb Cell 82\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m\"\u001b[39m, model_cie\u001b[39m.\u001b[39mscore(X_val_cie, y_val_cie))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMath\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model_mat \u001b[39m=\u001b[39m multinomial_logistic_regression(X_train_mat, y_train_mat, X_val_mat, y_val_mat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m\"\u001b[39m, model_mat\u001b[39m.\u001b[39mscore(X_val_mat, y_val_mat))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPortuguese\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32m/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb Cell 82\u001b[0m in \u001b[0;36mmultinomial_logistic_regression\u001b[0;34m(X_train, y_train, X_val, y_val, num_epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultinomial_logistic_regression\u001b[39m(X_train, y_train, X_val, y_val, num_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model \u001b[39m=\u001b[39m LogisticRegression(multi_class\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmultinomial\u001b[39m\u001b[39m'\u001b[39m, solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m, max_iter\u001b[39m=\u001b[39m num_epochs)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/unicamp/mc886/Copy_of_2022s2_mc886mo444_assignment_02.ipynb#Y134sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1589\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1588\u001b[0m     prefer \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprocesses\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1589\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m   1590\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m   1591\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1592\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49mprefer),\n\u001b[1;32m   1593\u001b[0m )(\n\u001b[1;32m   1594\u001b[0m     path_func(\n\u001b[1;32m   1595\u001b[0m         X,\n\u001b[1;32m   1596\u001b[0m         y,\n\u001b[1;32m   1597\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[1;32m   1598\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[1;32m   1599\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1600\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1601\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1602\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1603\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1604\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   1605\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1606\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1607\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1608\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1609\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[1;32m   1610\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m   1611\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   1612\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[1;32m   1614\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[1;32m   1615\u001b[0m )\n\u001b[1;32m   1617\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:864\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    861\u001b[0m         alpha \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m l1_ratio)\n\u001b[1;32m    862\u001b[0m         beta \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m l1_ratio\n\u001b[0;32m--> 864\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[39m=\u001b[39m sag_solver(\n\u001b[1;32m    865\u001b[0m         X,\n\u001b[1;32m    866\u001b[0m         target,\n\u001b[1;32m    867\u001b[0m         sample_weight,\n\u001b[1;32m    868\u001b[0m         loss,\n\u001b[1;32m    869\u001b[0m         alpha,\n\u001b[1;32m    870\u001b[0m         beta,\n\u001b[1;32m    871\u001b[0m         max_iter,\n\u001b[1;32m    872\u001b[0m         tol,\n\u001b[1;32m    873\u001b[0m         verbose,\n\u001b[1;32m    874\u001b[0m         random_state,\n\u001b[1;32m    875\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    876\u001b[0m         max_squared_sum,\n\u001b[1;32m    877\u001b[0m         warm_start_sag,\n\u001b[1;32m    878\u001b[0m         is_saga\u001b[39m=\u001b[39;49m(solver \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    883\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msolver must be one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m solver\n\u001b[1;32m    885\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:327\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent sag implementation does not handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    326\u001b[0m sag \u001b[39m=\u001b[39m sag64 \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64 \u001b[39melse\u001b[39;00m sag32\n\u001b[0;32m--> 327\u001b[0m num_seen, n_iter_ \u001b[39m=\u001b[39m sag(\n\u001b[1;32m    328\u001b[0m     dataset,\n\u001b[1;32m    329\u001b[0m     coef_init,\n\u001b[1;32m    330\u001b[0m     intercept_init,\n\u001b[1;32m    331\u001b[0m     n_samples,\n\u001b[1;32m    332\u001b[0m     n_features,\n\u001b[1;32m    333\u001b[0m     n_classes,\n\u001b[1;32m    334\u001b[0m     tol,\n\u001b[1;32m    335\u001b[0m     max_iter,\n\u001b[1;32m    336\u001b[0m     loss,\n\u001b[1;32m    337\u001b[0m     step_size,\n\u001b[1;32m    338\u001b[0m     alpha_scaled,\n\u001b[1;32m    339\u001b[0m     beta_scaled,\n\u001b[1;32m    340\u001b[0m     sum_gradient_init,\n\u001b[1;32m    341\u001b[0m     gradient_memory_init,\n\u001b[1;32m    342\u001b[0m     seen_init,\n\u001b[1;32m    343\u001b[0m     num_seen_init,\n\u001b[1;32m    344\u001b[0m     fit_intercept,\n\u001b[1;32m    345\u001b[0m     intercept_sum_gradient,\n\u001b[1;32m    346\u001b[0m     intercept_decay,\n\u001b[1;32m    347\u001b[0m     is_saga,\n\u001b[1;32m    348\u001b[0m     verbose,\n\u001b[1;32m    349\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m n_iter_ \u001b[39m==\u001b[39m max_iter:\n\u001b[1;32m    352\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    353\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    354\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    355\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TODO: Multinomial Logistic Regression. You can use scikit-learn libraries.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def multinomial_logistic_regression(X_train, y_train, X_val, y_val, num_epochs = 1000):\n",
        "    model = LogisticRegression(multi_class='multinomial', solver='sag', max_iter= num_epochs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "print(\"Science\")\n",
        "model_cie = multinomial_logistic_regression(X_train_cie, y_train_cie, X_val_cie, y_val_cie)\n",
        "print(\"Accuracy: \", model_cie.score(X_val_cie, y_val_cie))\n",
        "print(\"Math\")\n",
        "model_mat = multinomial_logistic_regression(X_train_mat, y_train_mat, X_val_mat, y_val_mat)\n",
        "print(\"Accuracy: \", model_mat.score(X_val_mat, y_val_mat))\n",
        "print(\"Portuguese\")\n",
        "model_lp = multinomial_logistic_regression(X_train_lp, y_train_lp, X_val_lp, y_val_lp)\n",
        "print(\"Accuracy: \", model_lp.score(X_val_lp, y_val_lp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQj3oImUUseT"
      },
      "source": [
        "> What are the conclusions? (1-2 paragraphs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb1KNEqLUseT"
      },
      "source": [
        "2. (0.5 point) Plot the cost function vs. number of epochs in the training/validation set and analyze the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfECeHi3UseT"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the cost function vs. number of iterations in the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IM4mx23UseT"
      },
      "source": [
        "> What are the conclusions? (1-2 paragraphs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqlv9-6OUseT"
      },
      "source": [
        "3. (0.75 point) Pick **your best model** and plot the confusion matrix in the **test set**. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jdyJuS0UseT"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the confusion matrix. You can use scikit-learn, seaborn, matplotlib libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmCj0cpUseT"
      },
      "source": [
        "> What are the conclusions? (1-2 paragraphs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSGS4brHnAi"
      },
      "source": [
        "## Deadline\n",
        "\n",
        "Monday, September 19, 11:59 pm. \n",
        "\n",
        "Penalty policy for late submission: You are not encouraged to submit your assignment after due date. However, in case you do, your grade will be penalized as follows:\n",
        "- September 20, 11:59 pm : grade * 0.75\n",
        "- September 21, 11:59 pm : grade * 0.5\n",
        "- September 22, 11:59 pm : grade * 0.25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joN9pvZJIfW5"
      },
      "source": [
        "## Submission\n",
        "\n",
        "On Google Classroom, submit your Jupyter Notebook (in Portuguese or English).\n",
        "\n",
        "**This activity is NOT individual, it must be done in pairs (two-person group).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
